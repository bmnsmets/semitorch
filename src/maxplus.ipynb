{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Max-Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.5.0, llvm 15.0.4, commit 7b885c28, linux, python 3.10.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 05/10/23 15:15:52.805 34838] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from semitorch.utils import Timer, CUDATimer, ntuple, mnistplot\n",
    "from typing import Optional, Union, Tuple, TypeVar\n",
    "import math\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "import hidet\n",
    "\n",
    "device = torch.device('cuda')\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "B, Dx, Dy = 2048, 1024, 512\n",
    "x = torch.rand(B, Dx, dtype=torch.float32, device=device, requires_grad=True)\n",
    "a = torch.randn(Dy, Dx, dtype=torch.float32, device=device, requires_grad=True)\n",
    "grad_y = torch.randn(B, Dy, dtype=torch.float32, device=device, requires_grad=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (version 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 47.65 ms\n",
      "Elapsed: 36.42 ms\n",
      "Elapsed: 35.73 ms\n"
     ]
    }
   ],
   "source": [
    "def maxplus_v0(x, a):\n",
    "    assert a.ndim == 2 and x.ndim >= 1\n",
    "    assert x.shape[-1] == a.shape[-1]\n",
    "    x = x.unsqueeze(-2)\n",
    "    return torch.max(x + a, dim=-1)[0]\n",
    "\n",
    "\n",
    "with CUDATimer():\n",
    "    with torch.no_grad():\n",
    "        y_v0 = maxplus_v0(x, a)\n",
    "\n",
    "with CUDATimer():\n",
    "    y_v0 = maxplus_v0(x, a)\n",
    "\n",
    "with CUDATimer():\n",
    "    y_v0.backward(grad_y)\n",
    "\n",
    "# with torch.profiler.profile(\n",
    "#     record_shapes=True, profile_memory=True, with_stack=True,\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "#         '../samples', worker_name='maxplus_v0'),\n",
    "# ) as prof:\n",
    "#     y_v0 = maxplus_v0(x, a)\n",
    "#     y_v0.backward(grad_y)\n",
    "\n",
    "grad_x, grad_a = x.grad, a.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1: Naive Taichi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ti.kernel\n",
    "def maxplus_kernel_v1(\n",
    "    y: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "    x: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    a: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "):\n",
    "    for b, i in y:\n",
    "        v = -tm.inf\n",
    "        for j in range(a.shape[-1]):\n",
    "            v = tm.max(v, x[b, j] + a[i, j])\n",
    "        y[b, i] = v\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_fw_kernel_v1(\n",
    "    y: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    x: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    a: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "):\n",
    "    for b, i in y:\n",
    "        v = -tm.inf\n",
    "        hit: ti.i32 = -1\n",
    "        for j in range(a.shape[0]):\n",
    "            w = x[b, j] + a[i, j]\n",
    "            if w > v:\n",
    "                v = w\n",
    "                hit = j\n",
    "        y[b, i] = v\n",
    "        hits[b, i] = hit\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_bw_x_kernel_v1(\n",
    "    gradx: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    grady: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_bw_a_kernel_v1(\n",
    "    grada: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    grady: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MaxPlusFunction_v1(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a):\n",
    "        assert x.device == a.device, \"inputs x and a should be on the same device but are on f{x.device} resp. f{a.device}\"\n",
    "        x = x.contiguous()\n",
    "        a = a.contiguous()\n",
    "\n",
    "        y = torch.empty((*x.shape[0:-1], a.shape[0]), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if (x.requires_grad or a.requires_grad) and torch.is_grad_enabled():\n",
    "            hits = torch.empty_like(x, dtype=torch.int32)\n",
    "            maxplus_fw_kernel_v1(y, hits, x, a)\n",
    "            ctx.save_for_backward(hits)\n",
    "        else:\n",
    "            maxplus_kernel_v1(y, x, a)\n",
    "\n",
    "        x.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        (hits,) = ctx.saved_tensors\n",
    "\n",
    "        grad_y.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "def maxplus_v1(x, a):\n",
    "    return MaxPlusFunction_v1.apply(x, a)\n",
    "\n",
    "\n",
    "with CUDATimer():\n",
    "    with torch.no_grad():\n",
    "        y_v1 = maxplus_v1(x, a)\n",
    "\n",
    "# with CUDATimer():\n",
    "#     y_v1 = maxplus_v1(x, a)\n",
    "#     y_v1.backward(grad_y)\n",
    "\n",
    "torch.allclose(y_v1, y_v0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Naive Hidet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxplus_fw_v2(nbatch, nin, nout):\n",
    "    from hidet.lang import f32, attr\n",
    "    from hidet.lang.cuda import threadIdx, blockIdx, blockDim\n",
    "\n",
    "    with hidet.script_module() as script_module:\n",
    "        @hidet.script\n",
    "        def kernel(\n",
    "            y: f32[nbatch, nout],\n",
    "            x: f32[nbatch, nout],\n",
    "            a: f32[nout, nin]\n",
    "        ):\n",
    "            attr.cuda_grid_dim = ((nbatch + 31) // 32, (nout + 31) // 32)\n",
    "            attr.cuda_block_dim = (32, 32)\n",
    "            i = threadIdx.x + blockIdx.x * blockDim.x\n",
    "            j = threadIdx.y + blockIdx.y * blockDim.y\n",
    "            if i < nbatch and j < nout:\n",
    "                val = -1e6\n",
    "                for k in range(nin):\n",
    "                    val = max(x[i, k] + a[j, k], val)\n",
    "                y[i, j] = val\n",
    "\n",
    "    ir_module = script_module.ir_module()\n",
    "    func = hidet.driver.build_ir_module(ir_module)\n",
    "    return func\n",
    "\n",
    "\n",
    "class MaxPlusFunction_v2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a):\n",
    "        assert x.device == a.device, \"inputs x and a should be on the same device but are on f{x.device} resp. f{a.device}\"\n",
    "        x = x.contiguous()\n",
    "        a = a.contiguous()\n",
    "\n",
    "        y = torch.empty((*x.shape[0:-1], a.shape[0]), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if (x.requires_grad or a.requires_grad) and torch.is_grad_enabled():\n",
    "            hits = torch.empty_like(x, dtype=torch.int32)\n",
    "            # maxplus_fw_kernel_v1(y, hits, x, a)\n",
    "            ctx.save_for_backward(hits)\n",
    "        else:\n",
    "            nbatch, nin = x.shape\n",
    "            nout, nin = a.shape\n",
    "            f = maxplus_fw_v2(nbatch, nin, nout)\n",
    "            f(y.detach(), x.detach(), a.detach())\n",
    "\n",
    "        x.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        pass\n",
    "\n",
    "\n",
    "def maxplus_v2(x, a):\n",
    "    return MaxPlusFunction_v2.apply(x, a)\n",
    "\n",
    "\n",
    "with CUDATimer():\n",
    "    with torch.no_grad():\n",
    "        y_v2 = maxplus_v2(x, a)\n",
    "\n",
    "# with CUDATimer():\n",
    "#     y_v1 = maxplus_v1(x, a)\n",
    "#     y_v1.backward(grad_y)\n",
    "\n",
    "torch.allclose(y_v2, y_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_v2 - y_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3: Rule-based Hidet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 34.90 ms\n",
      "Elapsed: 65.18 ms\n",
      "Elapsed: 3.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hidet.ir.compute import TensorNode, compute, reduce, arg_reduce\n",
    "from hidet.ir.task import Task\n",
    "from hidet.graph import Operator, Tensor\n",
    "from hidet.graph.ops.definitions.utils import input_like\n",
    "\n",
    "class MaxPlusNoGradTask(Task):\n",
    "    def __init__(self, x: TensorNode, a: TensorNode):\n",
    "        # get the input sizes\n",
    "        batch_size, in_size = x.const_shape()\n",
    "        out_size, in_size = a.const_shape()\n",
    "\n",
    "        # define the computation\n",
    "        y = compute(\n",
    "            name='y',\n",
    "            shape=[batch_size, out_size],\n",
    "            fcompute=lambda b, i: reduce(\n",
    "                shape=[in_size],\n",
    "                fcompute=lambda k: x[b, k] + a[i, k],\n",
    "                reduce_type='max',\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # call the parent class constructor to initialize the task\n",
    "        super().__init__(\n",
    "            name='maxplus',  # the name of the task\n",
    "            inputs=[x, a],  # the input tensor nodes\n",
    "            outputs=[y],  # the output tensor nodes\n",
    "        )\n",
    "\n",
    "class MaxPlusNoGradOp(Operator):\n",
    "    def __init__(self, x, a):\n",
    "        # call the parent class constructor to initialize the operator\n",
    "        super().__init__(\n",
    "            inputs=[x, a],  # the input tensors\n",
    "            attributes={},\n",
    "            task=MaxPlusNoGradTask(  # the task of the operator\n",
    "                # create tensor nodes (TensorNode) with the same shape and dtype as the tensors (Tensor)\n",
    "                input_like(x, 'x'),\n",
    "                input_like(a, 'a'),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "class MaxPlusTask(Task):\n",
    "    def __init__(self, x: TensorNode, a: TensorNode):\n",
    "        # get the input sizes\n",
    "        batch_size, in_size = x.const_shape()\n",
    "        out_size, in_size = a.const_shape()\n",
    "\n",
    "        # define the computation\n",
    "        y = compute(\n",
    "            name='y',\n",
    "            shape=[batch_size, out_size],\n",
    "            fcompute=lambda b, i: reduce(\n",
    "                shape=[in_size],\n",
    "                fcompute=lambda k: x[b, k] + a[i, k],\n",
    "                reduce_type='max',\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        hits = compute(\n",
    "            name='hits',\n",
    "            shape=[batch_size, out_size],\n",
    "            fcompute=lambda b, i: arg_reduce(\n",
    "                in_size,\n",
    "                fcompute=lambda k: x[b, k] + a[i, k],\n",
    "                reduce_type='max',\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # call the parent class constructor to initialize the task\n",
    "        super().__init__(\n",
    "            name='maxplus',  # the name of the task\n",
    "            inputs=[x, a],  # the input tensor nodes\n",
    "            outputs=[y, hits],  # the output tensor nodes\n",
    "        )\n",
    "\n",
    "class MaxPlusOp(Operator):\n",
    "    def __init__(self, x, a):\n",
    "        # call the parent class constructor to initialize the operator\n",
    "        super().__init__(\n",
    "            inputs=[x, a],  # the input tensors\n",
    "            attributes={},\n",
    "            task=MaxPlusTask(  # the task of the operator\n",
    "                # create tensor nodes (TensorNode) with the same shape and dtype as the tensors (Tensor)\n",
    "                input_like(x, 'x'),\n",
    "                input_like(a, 'a'),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "def maxplus_bw_x_v3(nbatch, nin, nout):\n",
    "    from hidet.lang import f32, i64, attr\n",
    "    from hidet.lang.cuda import threadIdx, blockIdx, blockDim\n",
    "\n",
    "    with hidet.script_module() as script_module:\n",
    "        @hidet.script\n",
    "        def kernel(\n",
    "            grad_x: f32[nbatch, nin],\n",
    "            grad_y: f32[nbatch, nout],\n",
    "            hits: i64[nbatch, nout]\n",
    "        ):\n",
    "            attr.cuda_grid_dim = ((nbatch + 31) // 32, (nin + 31) // 32)\n",
    "            attr.cuda_block_dim = (32, 32)\n",
    "            i = threadIdx.x + blockIdx.x * blockDim.x\n",
    "            j = threadIdx.y + blockIdx.y * blockDim.y\n",
    "            if i < nbatch and j < nin:\n",
    "                for k in range(nout):\n",
    "                    if hits[i, k] == j:\n",
    "                        grad_x[i, j] += grad_y[i, k]\n",
    "\n",
    "    ir_module = script_module.ir_module()\n",
    "    func = hidet.driver.build_ir_module(ir_module)\n",
    "    return func\n",
    "\n",
    "def maxplus_bw_a_v3(nbatch, nin, nout):\n",
    "    from hidet.lang import f32, i64, attr\n",
    "    from hidet.lang.cuda import threadIdx, blockIdx, blockDim\n",
    "\n",
    "    with hidet.script_module() as script_module:\n",
    "        @hidet.script\n",
    "        def kernel(\n",
    "            grad_a: f32[nout, nin],\n",
    "            grad_y: f32[nbatch, nout],\n",
    "            hits: i64[nbatch, nout]\n",
    "        ):\n",
    "            attr.cuda_grid_dim = ((nout + 31) // 32, (nin + 31) // 32)\n",
    "            attr.cuda_block_dim = (32, 32)\n",
    "            i = threadIdx.x + blockIdx.x * blockDim.x\n",
    "            j = threadIdx.y + blockIdx.y * blockDim.y\n",
    "            if i < nout and j < nin:\n",
    "                for k in range(nbatch):\n",
    "                    if hits[k, i] == j:\n",
    "                        grad_a[i, j] += grad_y[k, i]\n",
    "\n",
    "    ir_module = script_module.ir_module()\n",
    "    func = hidet.driver.build_ir_module(ir_module)\n",
    "    return func\n",
    "\n",
    "\n",
    "class MaxPlusFunction_v3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a, grad_enabled: bool):\n",
    "        assert x.device == a.device, \"inputs x and a should be on the same device but are on f{x.device} resp. f{a.device}\"\n",
    "        x = x.contiguous()\n",
    "        a = a.contiguous()\n",
    "\n",
    "        y = torch.empty((*x.shape[0:-1], a.shape[0]), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if grad_enabled:\n",
    "            op = MaxPlusOp(hidet.from_torch(x.detach()), hidet.from_torch(a.detach()))\n",
    "            y = op.get_output(0).torch()\n",
    "            hits = op.get_output(1).torch()\n",
    "            ctx.save_for_backward(hits)\n",
    "            ctx.in_features = x.shape[-1]\n",
    "        else:\n",
    "            op = MaxPlusNoGradOp(hidet.from_torch(x.detach()), hidet.from_torch(a.detach()))\n",
    "            y = op.get_output(0).torch()\n",
    "            \n",
    "        x.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        (hits,) = ctx.saved_tensors\n",
    "        grad_y = grad_y.contiguous()\n",
    "        hits.to(grad_y.device)\n",
    "\n",
    "        grad_x = torch.zeros(grad_y.shape[0], ctx.in_features, dtype=grad_y.dtype, device=grad_y.device)\n",
    "        grad_a = torch.zeros(grad_y.shape[1], ctx.in_features, dtype=grad_y.dtype, device=grad_y.device)\n",
    "\n",
    "        nbatch, nout = grad_y.shape\n",
    "        nin = ctx.in_features\n",
    "\n",
    "        backward_x = maxplus_bw_x_v3(nbatch, nin, nout)\n",
    "        backward_a = maxplus_bw_a_v3(nbatch, nin, nout)\n",
    "\n",
    "        backward_x(grad_x, grad_y, hits)\n",
    "        backward_a(grad_a, grad_y, hits)\n",
    "\n",
    "        grad_y.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "        return grad_x, grad_a, None\n",
    "\n",
    "\n",
    "def maxplus_v3(x, a):\n",
    "    return MaxPlusFunction_v3.apply(x, a, torch.is_grad_enabled())\n",
    "\n",
    "\n",
    "with CUDATimer():\n",
    "    with torch.no_grad():\n",
    "        y_v3 = maxplus_v3(x, a)\n",
    "\n",
    "with CUDATimer():\n",
    "    y_v3 = maxplus_v3(x, a)\n",
    "\n",
    "with CUDATimer():\n",
    "    x.grad, a.grad = None, None\n",
    "    y_v3.backward(grad_y)\n",
    "\n",
    "torch.allclose(y_v3, y_v0), torch.allclose(x.grad, grad_x, atol=1e-6), torch.allclose(a.grad, grad_a, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-298.7048, device='cuda:0'), 524288)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(grad_a - a.grad), torch.numel(grad_a - a.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch200cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
