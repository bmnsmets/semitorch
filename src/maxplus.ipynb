{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Max-Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.5.0, llvm 15.0.4, commit 7b885c28, linux, python 3.10.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 04/21/23 17:20:33.297 169584] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from semitorch.utils import Timer, CUDATimer, ntuple, mnistplot\n",
    "from typing import Optional, Union, Tuple, TypeVar\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\n",
    "device = torch.device('cuda')\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "B, Dx, Dy = 4096, 1024, 512\n",
    "x = torch.rand(B, Dx, dtype=torch.float32, device=device, requires_grad=False)\n",
    "a = torch.randn(Dy, Dx, dtype=torch.float32, device=device, requires_grad=False)\n",
    "grad_y = torch.randn(B, Dy, dtype=torch.float32, device=device, requires_grad=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 68.72 ms\n"
     ]
    }
   ],
   "source": [
    "def maxplus_v0(x, a):\n",
    "    assert a.ndim == 2 and x.ndim >= 1\n",
    "    assert x.shape[-1] == a.shape[-1]\n",
    "    x = x.unsqueeze(-2)\n",
    "    return torch.max(x + a, dim=-1)[0]\n",
    "\n",
    "\n",
    "with CUDATimer():\n",
    "    y_v0 = maxplus_v0(x, a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taichi v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 190.36 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ti.kernel\n",
    "def maxplus_kernel_v1(\n",
    "    y: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "    x: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    a: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "):\n",
    "    for b, i in y:\n",
    "        v = -tm.inf\n",
    "        for j in range(a.shape[-1]):\n",
    "            v = tm.max(v, x[b, j] + a[i, j])\n",
    "        y[b, i] = v\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_fw_kernel_v1(\n",
    "    y: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    x: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    a: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "):\n",
    "    for b, i in y:\n",
    "        v = -tm.inf\n",
    "        hit = -1\n",
    "        for j in range(a.shape[0]):\n",
    "            w = x[b, j] + a[i, j]\n",
    "            if w > v:\n",
    "                v = w\n",
    "                hit = j\n",
    "        y[b, i] = v\n",
    "        hits[b, i] = hit\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_bw_x_kernel_v1(\n",
    "    gradx: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    grady: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_bw_a_kernel_v1(\n",
    "    grada: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    grady: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MaxPlusFunction_v1(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a):\n",
    "        x = x.contiguous()\n",
    "        a = a.contiguous()\n",
    "\n",
    "        y = torch.empty((*x.shape[0:-1], a.shape[0]), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if x.requires_grad or a.requires_grad:\n",
    "            hits = torch.empty_like(x, dtype=torch.int32)\n",
    "            maxplus_fw_kernel_v1(y, hits, x, a)\n",
    "            ctx.save_for_backward(hits)\n",
    "        else:\n",
    "            maxplus_kernel_v1(y, x, a)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        (hits,) = ctx.saved_tensors\n",
    "\n",
    "        ti.sync()\n",
    "\n",
    "\n",
    "def maxplus_v1(x, a):\n",
    "    return MaxPlusFunction_v1.apply(x, a)\n",
    "\n",
    "\n",
    "with CUDATimer():\n",
    "    y_v1 = maxplus_v1(x, a)\n",
    "\n",
    "torch.allclose(y_v1, y_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch200cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
