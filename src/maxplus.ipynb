{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Max-Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.5.0, llvm 15.0.4, commit 7b885c28, linux, python 3.10.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 05/08/23 13:31:41.529 11300] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from semitorch.utils import Timer, CUDATimer, ntuple, mnistplot\n",
    "from typing import Optional, Union, Tuple, TypeVar\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\n",
    "device = torch.device('cuda')\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "B, Dx, Dy = 2048, 1024, 512\n",
    "x = torch.rand(B, Dx, dtype=torch.float32, device=device, requires_grad=True)\n",
    "a = torch.randn(Dy, Dx, dtype=torch.float32, device=device, requires_grad=True)\n",
    "grad_y = torch.randn(B, Dy, dtype=torch.float32, device=device, requires_grad=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (version 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 48.96 ms\n",
      "Elapsed: 36.49 ms\n",
      "Elapsed: 35.91 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-08 13:31:44 11300:11300 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-08 13:31:44 11300:11300 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-08 13:31:44 11300:11300 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "def maxplus_v0(x, a):\n",
    "    assert a.ndim == 2 and x.ndim >= 1\n",
    "    assert x.shape[-1] == a.shape[-1]\n",
    "    x = x.unsqueeze(-2)\n",
    "    return torch.max(x + a, dim=-1)[0]\n",
    "\n",
    "\n",
    "with CUDATimer():\n",
    "    with torch.no_grad():\n",
    "        y_v0 = maxplus_v0(x, a)\n",
    "\n",
    "with CUDATimer():\n",
    "    y_v0 = maxplus_v0(x, a)\n",
    "\n",
    "with CUDATimer():\n",
    "    y_v0.backward(grad_y)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    record_shapes=True, profile_memory=True, with_stack=True,\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "        '../samples', worker_name='maxplus_v0'),\n",
    ") as prof:\n",
    "    y_v0 = maxplus_v0(x, a)\n",
    "    y_v0.backward(grad_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1: Naive Taichi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 107.46 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ti.kernel\n",
    "def maxplus_kernel_v1(\n",
    "    y: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "    x: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    a: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "):\n",
    "    for b, i in y:\n",
    "        v = -tm.inf\n",
    "        for j in range(a.shape[-1]):\n",
    "            v = tm.max(v, x[b, j] + a[i, j])\n",
    "        y[b, i] = v\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_fw_kernel_v1(\n",
    "    y: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    x: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    a: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "):\n",
    "    for b, i in y:\n",
    "        v = -tm.inf\n",
    "        hit: ti.i32 = -1\n",
    "        for j in range(a.shape[0]):\n",
    "            w = x[b, j] + a[i, j]\n",
    "            if w > v:\n",
    "                v = w\n",
    "                hit = j\n",
    "        y[b, i] = v\n",
    "        hits[b, i] = hit\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_bw_x_kernel_v1(\n",
    "    gradx: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    grady: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_bw_a_kernel_v1(\n",
    "    grada: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dx]\n",
    "    grady: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MaxPlusFunction_v1(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a):\n",
    "        assert x.device == a.device, \"inputs x and a should be on the same device but are on f{x.device} resp. f{a.device}\"\n",
    "        x = x.contiguous()\n",
    "        a = a.contiguous()\n",
    "\n",
    "        y = torch.empty((*x.shape[0:-1], a.shape[0]), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if (x.requires_grad or a.requires_grad) and torch.is_grad_enabled():\n",
    "            hits = torch.empty_like(x, dtype=torch.int32)\n",
    "            maxplus_fw_kernel_v1(y, hits, x, a)\n",
    "            ctx.save_for_backward(hits)\n",
    "        else:\n",
    "            maxplus_kernel_v1(y, x, a)\n",
    "\n",
    "        x.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        (hits,) = ctx.saved_tensors\n",
    "\n",
    "        grad_y.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "def maxplus_v1(x, a):\n",
    "    return MaxPlusFunction_v1.apply(x, a)\n",
    "\n",
    "\n",
    "with CUDATimer():\n",
    "    with torch.no_grad():\n",
    "        y_v1 = maxplus_v1(x, a)\n",
    "\n",
    "# with CUDATimer():\n",
    "#     y_v1 = maxplus_v1(x, a)\n",
    "#     y_v1.backward(grad_y)\n",
    "\n",
    "torch.allclose(y_v1, y_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "x.device.type=='cuda' and print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch200cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
