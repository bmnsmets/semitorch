{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Max-Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/bsmetsjr/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/bsmetsjr/.cache/torch_extensions/py310_cu118/libsemitorch/build.ninja...\n",
      "Building extension module libsemitorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] c++ -MMD -MF maxplus.o.d -DTORCH_EXTENSION_NAME=libsemitorch -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/bsmetsjr/mambaforge/envs/pytorch20cu118/lib/python3.10/site-packages/torch/include -isystem /home/bsmetsjr/mambaforge/envs/pytorch20cu118/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/bsmetsjr/mambaforge/envs/pytorch20cu118/lib/python3.10/site-packages/torch/include/TH -isystem /home/bsmetsjr/mambaforge/envs/pytorch20cu118/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/bsmetsjr/mambaforge/envs/pytorch20cu118/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -DWITH_CUDA=1 -c /home/bsmetsjr/Documents/semitorch/src/semitorch/csrc/maxplus.cpp -o maxplus.o \n",
      "In file included from /home/bsmetsjr/Documents/semitorch/src/semitorch/csrc/maxplus.cpp:2:\n",
      "/home/bsmetsjr/Documents/semitorch/src/semitorch/csrc/dbg.h:39:76: note: ‘#pragma message: WARNING: the 'dbg.h' header is included in your code base’\n",
      "   39 | #pragma message(\"WARNING: the 'dbg.h' header is included in your code base\")\n",
      "      |                                                                            ^\n",
      "[2/2] c++ extension.o maxplus.o maxplus_cuda.cuda.o -shared -L/home/bsmetsjr/mambaforge/envs/pytorch20cu118/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o libsemitorch.so\n",
      "[Taichi] version 1.5.0, llvm 15.0.4, commit 7b885c28, linux, python 3.10.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module libsemitorch...\n",
      "[I 06/13/23 13:50:21.058 24772] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from semitorch.utils import Timer, CUDATimer, ntuple, mnistplot\n",
    "from typing import Optional, Union, Tuple, TypeVar\n",
    "import math\n",
    "\n",
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "import hidet\n",
    "\n",
    "import timeit\n",
    "\n",
    "device = torch.device('cuda')\n",
    "ti.init(arch=ti.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "B, Dx, Dy = 2048, 1024, 512\n",
    "x = torch.rand(B, Dx, dtype=torch.float32, device=device, requires_grad=True)\n",
    "a = torch.randn(Dy, Dx, dtype=torch.float32, device=device, requires_grad=True)\n",
    "grad_y = torch.randn(B, Dy, dtype=torch.float32, device=device, requires_grad=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (version 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxplus_v0(x, a):\n",
    "    assert a.ndim == 2 and x.ndim >= 1\n",
    "    assert x.shape[-1] == a.shape[-1]\n",
    "    x = x.unsqueeze(-2)\n",
    "    return torch.max(x + a, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.6 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1\n",
    "with torch.no_grad():\n",
    "    y_v0 = maxplus_v0(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.1 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1\n",
    "y_v0 = maxplus_v0(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1\n",
    "y_v0 = maxplus_v0(x, a)\n",
    "y_v0.backward(grad_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x, grad_a = x.grad, a.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1: Naive Taichi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ti.kernel\n",
    "def maxplus_kernel_v1(\n",
    "    y: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "    x: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    a: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "):\n",
    "    for b, i in y:\n",
    "        v = -tm.inf\n",
    "        for j in range(a.shape[-1]):\n",
    "            v = tm.max(v, x[b, j] + a[i, j])\n",
    "        y[b, i] = v\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_fw_kernel_v1(\n",
    "    y: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dy]\n",
    "    x: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    a: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "):\n",
    "    for b, i in y:\n",
    "        v = -tm.inf\n",
    "        hit: ti.i32 = -1\n",
    "        for j in range(a.shape[-1]):\n",
    "            w = x[b, j] + a[i, j]\n",
    "            if w > v:\n",
    "                v = w\n",
    "                hit = j\n",
    "        y[b, i] = v\n",
    "        hits[b, i] = hit\n",
    "\n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_bw_x_kernel_v1(\n",
    "    gradx: ti.types.ndarray(ndim=2),  # [B,Dx]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dy]\n",
    "    grady: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "):\n",
    "    for b, i in gradx:\n",
    "        gradx[b, i] = 0\n",
    "        for j in range(hits.shape[1]):\n",
    "            if hits[b, j] == i:\n",
    "                gradx[b, i] += grady[b, j]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "@ti.kernel\n",
    "def maxplus_bw_a_kernel_v1(\n",
    "    grada: ti.types.ndarray(ndim=2),  # [Dy,Dx]\n",
    "    hits: ti.types.ndarray(dtype=ti.i32, ndim=2),  # [B,Dy]\n",
    "    grady: ti.types.ndarray(ndim=2),  # [B,Dy]\n",
    "):\n",
    "    for j, i in grada:\n",
    "        grada[j, i] = 0\n",
    "        for b in range(hits.shape[0]):\n",
    "            if hits[b, j] == i:\n",
    "                grada[j, i] += grady[b, j]\n",
    "\n",
    "\n",
    "class MaxPlusFunction_v1(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a, grad_enabled=True):\n",
    "        assert x.device == a.device, \"inputs x and a should be on the same device but are on f{x.device} resp. f{a.device}\"\n",
    "        x = x.contiguous()\n",
    "        a = a.contiguous()\n",
    "\n",
    "        y = torch.empty((*x.shape[0:-1], a.shape[0]), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        ctx.a_shape = a.shape\n",
    "        ctx.x_shape = x.shape\n",
    "\n",
    "        if (x.requires_grad or a.requires_grad) and grad_enabled:\n",
    "            hits = torch.empty_like(x, dtype=torch.int32)\n",
    "            maxplus_fw_kernel_v1(y, hits, x, a)\n",
    "            ctx.save_for_backward(hits)\n",
    "        else:\n",
    "            maxplus_kernel_v1(y, x, a)\n",
    "\n",
    "        ti.sync()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grady):\n",
    "        hits, = ctx.saved_tensors\n",
    "\n",
    "        grada = torch.empty(ctx.a_shape, dtype=grady.dtype, device=grady.device)\n",
    "        gradx = torch.empty(ctx.x_shape, dtype=grady.dtype, device=grady.device)\n",
    "\n",
    "        maxplus_bw_a_kernel_v1(grada, hits, grady)\n",
    "        maxplus_bw_x_kernel_v1(gradx, hits, grady)\n",
    "\n",
    "        ti.sync()\n",
    "        return gradx, grada, None\n",
    "\n",
    "\n",
    "def maxplus_v1(x, a):\n",
    "    return MaxPlusFunction_v1.apply(x, a, torch.is_grad_enabled())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1\n",
    "with torch.no_grad():\n",
    "    y_v1 = maxplus_v1(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1\n",
    "y_v1 = maxplus_v1(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 1\n",
    "y_v1 = maxplus_v1(x, a)\n",
    "y_v1.backward(grad_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Naive Hidet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxplus_fw_v2(nbatch, nin, nout):\n",
    "    from hidet.lang import f32, attr\n",
    "    from hidet.lang.cuda import threadIdx, blockIdx, blockDim\n",
    "\n",
    "    with hidet.script_module() as script_module:\n",
    "        @hidet.script\n",
    "        def kernel(\n",
    "            y: f32[nbatch, nout],\n",
    "            x: f32[nbatch, nout],\n",
    "            a: f32[nout, nin]\n",
    "        ):\n",
    "            attr.cuda_grid_dim = ((nbatch + 31) // 32, (nout + 31) // 32)\n",
    "            attr.cuda_block_dim = (32, 32)\n",
    "            i = threadIdx.x + blockIdx.x * blockDim.x\n",
    "            j = threadIdx.y + blockIdx.y * blockDim.y\n",
    "            if i < nbatch and j < nout:\n",
    "                val = -1e6\n",
    "                for k in range(nin):\n",
    "                    val = max(x[i, k] + a[j, k], val)\n",
    "                y[i, j] = val\n",
    "\n",
    "    ir_module = script_module.ir_module()\n",
    "    func = hidet.driver.build_ir_module(ir_module)\n",
    "    return func\n",
    "\n",
    "\n",
    "class MaxPlusFunction_v2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a):\n",
    "        assert x.device == a.device, \"inputs x and a should be on the same device but are on f{x.device} resp. f{a.device}\"\n",
    "        x = x.contiguous()\n",
    "        a = a.contiguous()\n",
    "\n",
    "        y = torch.empty((*x.shape[0:-1], a.shape[0]), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if (x.requires_grad or a.requires_grad) and torch.is_grad_enabled():\n",
    "            hits = torch.empty_like(x, dtype=torch.int32)\n",
    "            # maxplus_fw_kernel_v1(y, hits, x, a)\n",
    "            ctx.save_for_backward(hits)\n",
    "        else:\n",
    "            nbatch, nin = x.shape\n",
    "            nout, nin = a.shape\n",
    "            f = maxplus_fw_v2(nbatch, nin, nout)\n",
    "            f(y.detach(), x.detach(), a.detach())\n",
    "\n",
    "        x.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        pass\n",
    "\n",
    "\n",
    "def maxplus_v2(x, a):\n",
    "    return MaxPlusFunction_v2.apply(x, a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.37 s ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10 -r 1\n",
    "with torch.no_grad():\n",
    "    y_v2 = maxplus_v2(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3: Rule-based Hidet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidet.ir.compute import TensorNode, compute, reduce, arg_reduce\n",
    "from hidet.ir.task import Task\n",
    "from hidet.graph import Operator, Tensor\n",
    "from hidet.graph.ops.definitions.utils import input_like\n",
    "\n",
    "class MaxPlusNoGradTask(Task):\n",
    "    def __init__(self, x: TensorNode, a: TensorNode):\n",
    "        # get the input sizes\n",
    "        batch_size, in_size = x.const_shape()\n",
    "        out_size, in_size = a.const_shape()\n",
    "\n",
    "        # define the computation\n",
    "        y = compute(\n",
    "            name='y',\n",
    "            shape=[batch_size, out_size],\n",
    "            fcompute=lambda b, i: reduce(\n",
    "                shape=[in_size],\n",
    "                fcompute=lambda k: x[b, k] + a[i, k],\n",
    "                reduce_type='max',\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # call the parent class constructor to initialize the task\n",
    "        super().__init__(\n",
    "            name='maxplus',  # the name of the task\n",
    "            inputs=[x, a],  # the input tensor nodes\n",
    "            outputs=[y],  # the output tensor nodes\n",
    "        )\n",
    "\n",
    "class MaxPlusNoGradOp(Operator):\n",
    "    def __init__(self, x, a):\n",
    "        # call the parent class constructor to initialize the operator\n",
    "        super().__init__(\n",
    "            inputs=[x, a],  # the input tensors\n",
    "            attributes={},\n",
    "            task=MaxPlusNoGradTask(  # the task of the operator\n",
    "                # create tensor nodes (TensorNode) with the same shape and dtype as the tensors (Tensor)\n",
    "                input_like(x, 'x'),\n",
    "                input_like(a, 'a'),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "class MaxPlusTask(Task):\n",
    "    def __init__(self, x: TensorNode, a: TensorNode):\n",
    "        # get the input sizes\n",
    "        batch_size, in_size = x.const_shape()\n",
    "        out_size, in_size = a.const_shape()\n",
    "\n",
    "        # define the computation\n",
    "        y = compute(\n",
    "            name='y',\n",
    "            shape=[batch_size, out_size],\n",
    "            fcompute=lambda b, i: reduce(\n",
    "                shape=[in_size],\n",
    "                fcompute=lambda k: x[b, k] + a[i, k],\n",
    "                reduce_type='max',\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        hits = compute(\n",
    "            name='hits',\n",
    "            shape=[batch_size, out_size],\n",
    "            fcompute=lambda b, i: arg_reduce(\n",
    "                in_size,\n",
    "                fcompute=lambda k: x[b, k] + a[i, k],\n",
    "                reduce_type='max',\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # call the parent class constructor to initialize the task\n",
    "        super().__init__(\n",
    "            name='maxplus',  # the name of the task\n",
    "            inputs=[x, a],  # the input tensor nodes\n",
    "            outputs=[y, hits],  # the output tensor nodes\n",
    "        )\n",
    "\n",
    "class MaxPlusOp(Operator):\n",
    "    def __init__(self, x, a):\n",
    "        # call the parent class constructor to initialize the operator\n",
    "        super().__init__(\n",
    "            inputs=[x, a],  # the input tensors\n",
    "            attributes={},\n",
    "            task=MaxPlusTask(  # the task of the operator\n",
    "                # create tensor nodes (TensorNode) with the same shape and dtype as the tensors (Tensor)\n",
    "                input_like(x, 'x'),\n",
    "                input_like(a, 'a'),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "def maxplus_bw_x_v3(nbatch, nin, nout):\n",
    "    from hidet.lang import f32, i64, attr\n",
    "    from hidet.lang.cuda import threadIdx, blockIdx, blockDim\n",
    "\n",
    "    with hidet.script_module() as script_module:\n",
    "        @hidet.script\n",
    "        def kernel(\n",
    "            grad_x: f32[nbatch, nin],\n",
    "            grad_y: f32[nbatch, nout],\n",
    "            hits: i64[nbatch, nout]\n",
    "        ):\n",
    "            attr.cuda_grid_dim = ((nbatch + 31) // 32, (nin + 31) // 32)\n",
    "            attr.cuda_block_dim = (32, 32)\n",
    "            i = threadIdx.x + blockIdx.x * blockDim.x\n",
    "            j = threadIdx.y + blockIdx.y * blockDim.y\n",
    "            if i < nbatch and j < nin:\n",
    "                for k in range(nout):\n",
    "                    if hits[i, k] == j:\n",
    "                        grad_x[i, j] += grad_y[i, k]\n",
    "\n",
    "    ir_module = script_module.ir_module()\n",
    "    func = hidet.driver.build_ir_module(ir_module)\n",
    "    return func\n",
    "\n",
    "def maxplus_bw_a_v3(nbatch, nin, nout):\n",
    "    from hidet.lang import f32, i64, attr\n",
    "    from hidet.lang.cuda import threadIdx, blockIdx, blockDim\n",
    "\n",
    "    with hidet.script_module() as script_module:\n",
    "        @hidet.script\n",
    "        def kernel(\n",
    "            grad_a: f32[nout, nin],\n",
    "            grad_y: f32[nbatch, nout],\n",
    "            hits: i64[nbatch, nout]\n",
    "        ):\n",
    "            attr.cuda_grid_dim = ((nout + 31) // 32, (nin + 31) // 32)\n",
    "            attr.cuda_block_dim = (32, 32)\n",
    "            i = threadIdx.x + blockIdx.x * blockDim.x\n",
    "            j = threadIdx.y + blockIdx.y * blockDim.y\n",
    "            if i < nout and j < nin:\n",
    "                for k in range(nbatch):\n",
    "                    if hits[k, i] == j:\n",
    "                        grad_a[i, j] += grad_y[k, i]\n",
    "\n",
    "    ir_module = script_module.ir_module()\n",
    "    func = hidet.driver.build_ir_module(ir_module)\n",
    "    return func\n",
    "\n",
    "\n",
    "class MaxPlusFunction_v3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a, grad_enabled: bool):\n",
    "        assert x.device == a.device, \"inputs x and a should be on the same device but are on f{x.device} resp. f{a.device}\"\n",
    "        x = x.contiguous()\n",
    "        a = a.contiguous()\n",
    "\n",
    "        y = torch.empty((*x.shape[0:-1], a.shape[0]), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if grad_enabled:\n",
    "            op = MaxPlusOp(hidet.from_torch(x.detach()), hidet.from_torch(a.detach()))\n",
    "            y = op.get_output(0).torch()\n",
    "            hits = op.get_output(1).torch()\n",
    "            ctx.save_for_backward(hits)\n",
    "            ctx.in_features = x.shape[-1]\n",
    "        else:\n",
    "            op = MaxPlusNoGradOp(hidet.from_torch(x.detach()), hidet.from_torch(a.detach()))\n",
    "            y = op.get_output(0).torch()\n",
    "            \n",
    "        x.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        (hits,) = ctx.saved_tensors\n",
    "        grad_y = grad_y.contiguous()\n",
    "        hits.to(grad_y.device)\n",
    "\n",
    "        grad_x = torch.zeros(grad_y.shape[0], ctx.in_features, dtype=grad_y.dtype, device=grad_y.device)\n",
    "        grad_a = torch.zeros(grad_y.shape[1], ctx.in_features, dtype=grad_y.dtype, device=grad_y.device)\n",
    "\n",
    "        nbatch, nout = grad_y.shape\n",
    "        nin = ctx.in_features\n",
    "\n",
    "        backward_x = maxplus_bw_x_v3(nbatch, nin, nout)\n",
    "        backward_a = maxplus_bw_a_v3(nbatch, nin, nout)\n",
    "\n",
    "        backward_x(grad_x, grad_y, hits)\n",
    "        backward_a(grad_a, grad_y, hits)\n",
    "\n",
    "        grad_y.device.type == 'cuda' and torch.cuda.synchronize()\n",
    "        return grad_x, grad_a, None\n",
    "\n",
    "\n",
    "def maxplus_v3(x, a):\n",
    "    return MaxPlusFunction_v3.apply(x, a, torch.is_grad_enabled())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10 -r 1\n",
    "with torch.no_grad():\n",
    "    y_v3 = maxplus_v3(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-298.7048, device='cuda:0'), 524288)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(grad_a - a.grad), torch.numel(grad_a - a.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch200cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
