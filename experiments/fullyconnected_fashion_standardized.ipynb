{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Experiments on FashionMNIST with a standardized setup\n",
    "\n",
    "\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "\n",
    "from filelock import FileLock\n",
    "from IPython import display\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from semitorch import MaxPlus, maxplus_parameters, nonmaxplus_parameters\n",
    "from semitorch import MinPlus, minplus_parameters, nonminplus_parameters\n",
    "from semitorch import TropicalSGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "data_path = os.path.abspath(\"./data\" if os.path.isdir(\"./data\") else \"../data\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FashionMNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 8\n",
    "\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.286,), (0.353,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.286,), (0.353,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = FashionMNIST(root=\".\", train=True, download=True, transform=transforms_train)\n",
    "testset = FashionMNIST(root=\".\", train=False, download=True, transform=transforms_test)\n",
    "\n",
    "\n",
    "def get_data_loaders():\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name: str, layer_norm: bool = False, skip_connections: bool = False, k=None) -> None:\n",
    "        super().__init__()\n",
    "        self.name = model_name\n",
    "        self.skip_connections = skip_connections\n",
    "\n",
    "        self.stem = nn.Sequential(*filter(lambda layer: layer is not None, [\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=4, stride=4),\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(8 * 7 * 7) if layer_norm else None,\n",
    "        ]))\n",
    "\n",
    "        if model_name == \"linear/relu\":\n",
    "            self.backbone_1 = nn.Sequential(\n",
    "                nn.Linear(8 * 7 * 7, 300), nn.ReLU(),\n",
    "                nn.Linear(300, 250), nn.ReLU(),\n",
    "            )\n",
    "            self.backbone_2 = nn.Sequential(\n",
    "                nn.Linear(250 + (8 * 7 * 7 if self.skip_connections else 0), 200), nn.ReLU(),\n",
    "                nn.Linear(200, 150), nn.ReLU(),\n",
    "            )\n",
    "            self.backbone_3 = nn.Sequential(\n",
    "                nn.Linear(150 + (250 if self.skip_connections else 0), 100), nn.ReLU(),\n",
    "                nn.Linear(100, 50), nn.ReLU(),\n",
    "            )\n",
    "        elif model_name == \"linear/maxplus\":\n",
    "            self.backbone_1 = nn.Sequential(\n",
    "                nn.Linear(8 * 7 * 7, 300),\n",
    "                MaxPlus(300, 250, k=k),\n",
    "            )\n",
    "            self.backbone_2 = nn.Sequential(\n",
    "                nn.Linear(250 + (8 * 7 * 7 if self.skip_connections else 0), 200),\n",
    "                MaxPlus(200, 150, k=k),\n",
    "            )\n",
    "            self.backbone_3 = nn.Sequential(\n",
    "                nn.Linear(150 + (250 if self.skip_connections else 0), 100),\n",
    "                MaxPlus(100, 50, k=k),\n",
    "            )\n",
    "        elif model_name == \"linear/minplus\":\n",
    "            self.backbone_1 = nn.Sequential(\n",
    "                nn.Linear(8 * 7 * 7, 300),\n",
    "                MinPlus(300, 250, k=k),\n",
    "            )\n",
    "            self.backbone_2 = nn.Sequential(\n",
    "                nn.Linear(250 + (8 * 7 * 7 if self.skip_connections else 0), 200),\n",
    "                MinPlus(200, 150, k=k),\n",
    "            )\n",
    "            self.backbone_3 = nn.Sequential(\n",
    "                nn.Linear(150 + (250 if self.skip_connections else 0), 100),\n",
    "                MinPlus(100, 50, k=k),\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown model ({model_name})\")\n",
    "\n",
    "        self.head = nn.Linear(50, 10, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        input_0 = x\n",
    "        result_0 = self.stem(input_0)\n",
    "\n",
    "        input_1 = result_0\n",
    "        result_1 = self.backbone_1(input_1)\n",
    "\n",
    "        if self.skip_connections:\n",
    "            input_2 = torch.cat((result_1, result_0), dim=-1)\n",
    "        else:\n",
    "            input_2 = result_1\n",
    "        result_2 = self.backbone_2(input_2)\n",
    "\n",
    "        if self.skip_connections:\n",
    "            input_3 = torch.cat((result_2, result_1), dim=-1)\n",
    "        else:\n",
    "            input_3 = result_2\n",
    "        result_3 = self.backbone_3(input_3)\n",
    "\n",
    "        output = self.head(result_3)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model: nn.Module, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    with torch.no_grad():\n",
    "        yout = model(x)\n",
    "        _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "\n",
    "        return (y.cpu() == prediction).sum().item() / float(y.numel())\n",
    "\n",
    "\n",
    "def test(model: nn.Module, device: str, testloader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x = x.to(device)\n",
    "            accs.append(accuracy(model, x, y))\n",
    "\n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "\n",
    "def confusion_matrix(model: nn.Module, device: str, testloader: DataLoader) -> None:\n",
    "    model.eval()\n",
    "\n",
    "    conf_matrix = torch.zeros(len(testset.classes), len(testset.classes))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            yout = model(x)\n",
    "            _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "\n",
    "            conf_matrix[y.cpu(), prediction] += 1\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    df_cm = pd.DataFrame(conf_matrix, index=testset.classes, columns=testset.classes).astype(int)\n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        device: str,\n",
    "        trainloader: DataLoader,\n",
    "        testloader: DataLoader,\n",
    "        optimizers: list[torch.optim.Optimizer],\n",
    "        schedulers: list[torch.optim.lr_scheduler],\n",
    "        loss: torch.nn.modules.loss,\n",
    "        epochs: int,\n",
    ") -> None:\n",
    "    accs = []  # list of accuracy on the test dataset for every epoch\n",
    "    trainaccs = []  # a list of the accuracies of all the training batches\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[6, 4])\n",
    "    hdisplay = display.display(\"\", display_id=True)\n",
    "\n",
    "    for _ in trange(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for x, y in trainloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.zero_grad()\n",
    "            yout = model(x)\n",
    "            _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "            trainaccs.append((y.cpu() == prediction).sum().item() / float(y.numel()))\n",
    "\n",
    "            l = loss(yout, y.squeeze())\n",
    "            l.backward()\n",
    "            for optimizer in optimizers:\n",
    "                if isinstance(optimizer, TropicalSGD):\n",
    "                    optimizer.step(input_tensor=x.cpu())\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "            for scheduler in schedulers:\n",
    "                scheduler.step()\n",
    "\n",
    "        accs.append(test(model, device, testloader))\n",
    "\n",
    "        ax.clear()\n",
    "        ax.set_xlim(0, epochs)\n",
    "        ax.set_ylim(-0.02, 1.02)\n",
    "        ax.plot(\n",
    "            np.linspace(0, len(accs), len(trainaccs)),\n",
    "            trainaccs,\n",
    "            \".\",\n",
    "            markersize=1.5,\n",
    "            markerfacecolor=(0, 0, 1, 0.3),\n",
    "        )\n",
    "        ax.plot(np.linspace(1, len(accs), len(accs)), accs)\n",
    "        ax.text(\n",
    "            0.6 * epochs,\n",
    "            0.30,\n",
    "            f\"max test acc = {max(accs):.2%}\",\n",
    "            ha=\"center\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "        hdisplay.update(fig)\n",
    "\n",
    "        # prevents OOM when GPU memory is tight\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    confusion_matrix(model, device, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_ray_tune(config: dict):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    epochs = 20\n",
    "    trainloader, testloader = get_data_loaders()\n",
    "\n",
    "    # Create model\n",
    "    model_name = config[\"model_name\"]\n",
    "    layer_norm = config[\"layer_norm\"]\n",
    "    skip_connections = config[\"skip_connections\"]\n",
    "    k = config[\"k\"]\n",
    "\n",
    "    model = Model(model_name=model_name, layer_norm=layer_norm, skip_connections=skip_connections, k=k).to(device)\n",
    "\n",
    "    # Separate model parameters\n",
    "    if model_name == \"linear/relu\":\n",
    "        linear_params = model.parameters()\n",
    "        semiring_params = nn.ParameterList()\n",
    "    elif model_name == \"linear/maxplus\":\n",
    "        linear_params = nonmaxplus_parameters(model)\n",
    "        semiring_params = maxplus_parameters(model)\n",
    "    elif model_name == \"linear/minplus\":\n",
    "        linear_params = nonminplus_parameters(model)\n",
    "        semiring_params = minplus_parameters(model)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown model ({model_name})\")\n",
    "\n",
    "    # Create linear optimizer\n",
    "    linear_lr = config[\"linear_lr\"]\n",
    "    if config[\"linear_optimizer\"] == \"AdamW\":\n",
    "        linear_optimizer = torch.optim.AdamW(linear_params, lr=linear_lr, weight_decay=0.01)\n",
    "    elif config[\"linear_optimizer\"] == \"SGD\":\n",
    "        linear_optimizer = torch.optim.SGD(linear_params, lr=linear_lr)\n",
    "    else:\n",
    "        raise RuntimeError(f'Unknown linear optimizer {config[\"linear_optimizer\"]}')\n",
    "    if config[\"linear_scheduler\"]:\n",
    "        linear_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            linear_optimizer,\n",
    "            max_lr=linear_lr,\n",
    "            anneal_strategy=\"linear\",\n",
    "            pct_start=0.3,\n",
    "            three_phase=True,\n",
    "            final_div_factor=1000.0,\n",
    "            div_factor=10.0,\n",
    "            steps_per_epoch=len(trainloader),\n",
    "            epochs=epochs,\n",
    "        )\n",
    "    else:\n",
    "        linear_scheduler = None\n",
    "\n",
    "    # Create semiring optimizer\n",
    "    semiring_lr = config[\"semiring_lr\"]\n",
    "    if config[\"semiring_optimizer\"] is None:\n",
    "        semiring_optimizer = None\n",
    "    elif config[\"semiring_optimizer\"] == \"SGD\":\n",
    "        semiring_optimizer = torch.optim.SGD(semiring_params, lr=semiring_lr)\n",
    "    elif config[\"semiring_optimizer\"] == \"TropicalSGD\":\n",
    "        semiring_optimizer = TropicalSGD(semiring_params, lr=semiring_lr)\n",
    "    else:\n",
    "        raise RuntimeError(f'Unknown semiring optimizer {config[\"semiring_optimizer\"]}')\n",
    "    if config[\"semiring_scheduler\"]:\n",
    "        semiring_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            semiring_optimizer,\n",
    "            max_lr=semiring_lr,\n",
    "            anneal_strategy=\"linear\",\n",
    "            pct_start=0.3,\n",
    "            three_phase=True,\n",
    "            final_div_factor=1000.0,\n",
    "            div_factor=10.0,\n",
    "            steps_per_epoch=len(trainloader),\n",
    "            epochs=epochs,\n",
    "        )\n",
    "    else:\n",
    "        semiring_scheduler = None\n",
    "\n",
    "    # Create optimizers and schedulers\n",
    "    optimizers = filter(lambda opt: opt is not None, [linear_optimizer, semiring_optimizer])\n",
    "    schedulers = filter(lambda sch: sch is not None, [linear_scheduler, semiring_scheduler])\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Feed to training function\n",
    "        reported_loss, reported_accuracy = train_ray_tune(\n",
    "            model,\n",
    "            device,\n",
    "            trainloader,\n",
    "            testloader,\n",
    "            optimizers,\n",
    "            schedulers,\n",
    "            loss,\n",
    "        )\n",
    "\n",
    "        session.report({\"loss\": reported_loss, \"accuracy\": reported_accuracy})\n",
    "\n",
    "\n",
    "def train_ray_tune(\n",
    "        model: nn.Module,\n",
    "        device: str,\n",
    "        trainloader: DataLoader,\n",
    "        testloader: DataLoader,\n",
    "        optimizers: list[torch.optim.Optimizer],\n",
    "        schedulers: list[torch.optim.lr_scheduler],\n",
    "        loss: torch.nn.modules.loss,\n",
    ") -> tuple[float, float]:\n",
    "    model.train()\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        l = loss(model(x), y.squeeze())\n",
    "        l.backward()\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            if isinstance(optimizer, TropicalSGD):\n",
    "                optimizer.step(input_tensor=x.cpu())\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "        for scheduler in schedulers:\n",
    "            scheduler.step()\n",
    "\n",
    "        test(model, device, testloader)\n",
    "\n",
    "        # prevents OOM when GPU memory is tight\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    total, correct, loss_val = 0, 0, 0\n",
    "    for x, y in testloader:\n",
    "        with torch.no_grad():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            yout = model(x)\n",
    "            _, predicted = torch.max(yout.data, dim=1)\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            loss_val += loss(yout, y)\n",
    "\n",
    "    return loss_val / len(testloader), correct / total\n",
    "\n",
    "\n",
    "def ray_find_best_model_for(config):\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(run_ray_tune),\n",
    "            resources={\"gpu\": 0.5, \"cpu\": 1}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            scheduler=ASHAScheduler(),\n",
    "            num_samples=1,\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=RunConfig(\n",
    "            verbose=0,\n",
    "            sync_config=tune.SyncConfig(\n",
    "                syncer=None,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    best_result = results.get_best_result(\"accuracy\", \"max\")\n",
    "    print(f\"Best trial config: {best_result.config}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Default Linear Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ray_find_best_model_for({\n",
    "    \"model_name\": \"linear/relu\",\n",
    "    \"layer_norm\": tune.choice([True, False]),\n",
    "    \"skip_connections\": tune.choice([True, False]),\n",
    "    \"k\": None,\n",
    "    \"linear_optimizer\": tune.choice([\"AdamW\", \"SGD\"]),\n",
    "    \"linear_scheduler\": tune.choice([True, False]),\n",
    "    \"linear_lr\": tune.loguniform(1e-6, 1),\n",
    "    \"semiring_lr\": None,\n",
    "    \"semiring_optimizer\": None,\n",
    "    \"semiring_scheduler\": None,\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best linear model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_linear_model = Model(\"linear/relu\").to(device)\n",
    "print(f\"{best_linear_model.name} model has {len(list(best_linear_model.params()))} trainable parameters\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "trainloader, testloader = get_data_loaders()\n",
    "\n",
    "best_linear_optimizer = torch.optim.AdamW(best_linear_model.parameters(), lr=lr1, weight_decay=0.01)\n",
    "best_linear_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    best_linear_optimizer,\n",
    "    max_lr=lr1,\n",
    "    anneal_strategy=\"linear\",\n",
    "    pct_start=0.3,\n",
    "    three_phase=True,\n",
    "    final_div_factor=1000.0,\n",
    "    div_factor=10.0,\n",
    "    steps_per_epoch=len(trainloader),\n",
    "    epochs=epochs,\n",
    ")\n",
    "\n",
    "train(\n",
    "    best_linear_model,\n",
    "    device,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    [best_linear_optimizer],\n",
    "    [best_linear_scheduler],\n",
    "    loss,\n",
    "    epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tropical models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MaxPlus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ray_find_best_model_for({\n",
    "    \"model_name\": \"linear/maxplus\",\n",
    "    \"layer_norm\": tune.choice([True, False]),\n",
    "    \"skip_connections\": tune.choice([True, False]),\n",
    "    \"k\": tune.uniform(-10, -1),\n",
    "    \"linear_optimizer\": tune.choice([\"AdamW\", \"SGD\"]),\n",
    "    \"linear_scheduler\": tune.choice([True, False]),\n",
    "    \"linear_lr\": tune.loguniform(1e-6, 1),\n",
    "    \"semiring_lr\": tune.loguniform(1e-6, 1),\n",
    "    \"semiring_optimizer\": tune.choice([\"SGD\", \"TropicalSGD\"]),\n",
    "    \"semiring_scheduler\": tune.choice([True, False]),\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best MaxPlus model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_maxplus_model = Model(\"linear/maxplus\").to(device)\n",
    "\n",
    "print(f\"{best_maxplus_model.name} model has {len(list(best_maxplus_model.parameters()))} trainable parameters, \"\n",
    "      f\"of which {len(list(nonmaxplus_parameters(best_maxplus_model)))} are linear \"\n",
    "      f\"and {len(list(maxplus_parameters(best_maxplus_model)))} are semiring related\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "trainloader, testloader = get_data_loaders()\n",
    "\n",
    "best_maxplus_linear_optimizer = torch.optim.SGD(nonmaxplus_parameters(best_maxplus_model), lr=lr1)\n",
    "best_maxplus_linear_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    best_maxplus_linear_optimizer,\n",
    "    max_lr=lr1,\n",
    "    anneal_strategy=\"linear\",\n",
    "    pct_start=0.3,\n",
    "    three_phase=True,\n",
    "    final_div_factor=1000.0,\n",
    "    div_factor=10.0,\n",
    "    steps_per_epoch=len(trainloader),\n",
    "    epochs=epochs,\n",
    ")\n",
    "best_maxplus_semiring_optimizer = torch.optim.SGD(maxplus_parameters(best_maxplus_model), lr=lr2)\n",
    "\n",
    "best_maxplus_optimizers = [best_maxplus_linear_optimizer, best_maxplus_semiring_optimizer]\n",
    "best_maxplus_schedulers = [best_maxplus_linear_scheduler]\n",
    "\n",
    "train(\n",
    "    best_maxplus_model,\n",
    "    device,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    best_maxplus_optimizers,\n",
    "    best_maxplus_schedulers,\n",
    "    loss,\n",
    "    epochs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MinPlus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ray_find_best_model_for({\n",
    "    \"model_name\": \"linear/minplus\",\n",
    "    \"layer_norm\": tune.choice([True, False]),\n",
    "    \"skip_connections\": tune.choice([True, False]),\n",
    "    \"k\": tune.uniform(1, 10),\n",
    "    \"linear_optimizer\": tune.choice([\"AdamW\", \"SGD\"]),\n",
    "    \"linear_scheduler\": tune.choice([True, False]),\n",
    "    \"linear_lr\": tune.loguniform(1e-6, 1),\n",
    "    \"semiring_lr\": tune.loguniform(1e-6, 1),\n",
    "    \"semiring_optimizer\": tune.choice([\"SGD\", \"TropicalSGD\"]),\n",
    "    \"semiring_scheduler\": tune.choice([True, False]),\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best MinPlus model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_minplus_model = Model(\"linear/minplus\").to(device)\n",
    "\n",
    "print(f\"{best_minplus_model.name} model has {len(list(best_minplus_model.parameters()))} trainable parameters, \"\n",
    "      f\"of which {len(list(nonminplus_parameters(best_minplus_model)))} are linear \"\n",
    "      f\"and {len(list(minplus_parameters(best_minplus_model)))} are semiring related\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "trainloader, testloader = get_data_loaders()\n",
    "\n",
    "best_minplus_linear_optimizer = torch.optim.SGD(nonminplus_parameters(best_minplus_model), lr=lr1)\n",
    "best_minplus_linear_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    best_minplus_linear_optimizer,\n",
    "    max_lr=lr1,\n",
    "    anneal_strategy=\"linear\",\n",
    "    pct_start=0.3,\n",
    "    three_phase=True,\n",
    "    final_div_factor=1000.0,\n",
    "    div_factor=10.0,\n",
    "    steps_per_epoch=len(trainloader),\n",
    "    epochs=epochs,\n",
    ")\n",
    "best_minplus_semiring_optimizer = torch.optim.SGD(minplus_parameters(best_minplus_model), lr=lr2)\n",
    "\n",
    "best_minplus_optimizers = [best_minplus_linear_optimizer, best_minplus_semiring_optimizer]\n",
    "best_minplus_schedulers = [best_minplus_linear_scheduler]\n",
    "\n",
    "train(\n",
    "    best_maxplus_model,\n",
    "    device,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    best_minplus_optimizers,\n",
    "    best_minplus_schedulers,\n",
    "    loss,\n",
    "    epochs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
