{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNeXt on FashionMNIST\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "convnext_st_classic_atto has 3,700,160 parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import timm.data\n",
    "import semitorch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "convnext_st_classic_atto = timm.create_model(\n",
    "    \"convnext_st_classic_atto\", in_chans=1, stem_patch_size=1\n",
    ")\n",
    "convnext_st_classic_atto = convnext_st_classic_atto.to(device)\n",
    "\n",
    "num_params = sum(\n",
    "    p.numel() for p in convnext_st_classic_atto.parameters() if p.requires_grad\n",
    ")\n",
    "print(f\"convnext_st_classic_atto has {num_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FashionMNIST dataset\n",
    "batch_size = 256\n",
    "num_workers = 8\n",
    "\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.286,),(0.353,)),\n",
    "        #transforms.RandomCrop((24, 24)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.286,),(0.353,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = FashionMNIST(root=\".\", train=True, download=True, transform=transforms_train)\n",
    "testset = FashionMNIST(root=\".\", train=False, download=True, transform=transforms_test)\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train the network with SGD+momentum under the [one cycle policy](https://arxiv.org/abs/1708.07120).\n",
    "As usual we use cross entropy loss.\n",
    "To track the track the training process we plot the accuracy of each training batch as a blue dot, every epoch we also test the network and the training data and plot the network's accuracy as an orange line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y):\n",
    "    with torch.no_grad():\n",
    "        yout = model(x)\n",
    "        _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "        return (y==prediction).sum().item() / float(y.numel())\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, testloader):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x = x.to(device)\n",
    "            accs.append(accuracy(model, x, y))\n",
    "    return sum(accs)/len(accs)\n",
    "\n",
    "\n",
    "def train(model, device, trainloader, testloader, optimizer, scheduler, loss, epochs):\n",
    "    accs = [] # list of accuracy on the test dataset for every epoch\n",
    "    trainaccs = [] # a list of the accuracies of all the training batches\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[6,4])\n",
    "    hdisplay = display.display(\"\", display_id=True)\n",
    "\n",
    "    for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        for x, y in trainloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yout = model(x)\n",
    "            _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "            trainaccs.append((y.cpu()==prediction).sum().item() / float(y.numel()))\n",
    "            l = loss(yout, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        accs.append(test(model, device, testloader))\n",
    "\n",
    "        ax.clear()\n",
    "        ax.set_xlim(0, epochs)\n",
    "        ax.set_ylim(0.75, 1.0)\n",
    "        ax.plot(np.linspace(0,len(accs),len(trainaccs)), \n",
    "            trainaccs, \".\", markersize=1.5, markerfacecolor=(0, 0, 1, 0.3))\n",
    "        ax.plot(np.linspace(1,len(accs),len(accs)), accs)\n",
    "        ax.text(0.6*epochs, 0.80, f\"max test acc = {max(accs):.2%}\", ha=\"center\", fontsize=10)\n",
    "        hdisplay.update(fig)\n",
    "        \n",
    "        # prevents OOM when GPU memory is tight\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    plt.close(fig)\n",
    "        \n",
    "\n",
    "def resetmodel(model: nn.Module) -> None:\n",
    "    @torch.no_grad()\n",
    "    def weight_reset(m: nn.Module):\n",
    "        reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "        if callable(reset_parameters):\n",
    "            m.reset_parameters()\n",
    "    model.apply(fn=weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convnext_st_classic_atto.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b4c77991024c2f9da03747fa3c5efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(convnext_st_classic_atto\u001b[39m.\u001b[39mparameters(), \n\u001b[1;32m      5\u001b[0m                 lr\u001b[39m=\u001b[39m\u001b[39m1.0e-3\u001b[39m,\n\u001b[1;32m      6\u001b[0m                 momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m,\n\u001b[1;32m      7\u001b[0m                 weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m\n\u001b[1;32m      8\u001b[0m             )\n\u001b[1;32m      9\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mOneCycleLR(optimizer, \n\u001b[1;32m     10\u001b[0m                 max_lr\u001b[39m=\u001b[39m\u001b[39m1.0e-3\u001b[39m,\n\u001b[1;32m     11\u001b[0m                 anneal_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 epochs\u001b[39m=\u001b[39mepochs\n\u001b[1;32m     18\u001b[0m             )\n\u001b[0;32m---> 20\u001b[0m train(convnext_st_classic_atto, device, trainloader, testloader, optimizer, scheduler, loss, epochs)\n",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, trainloader, testloader, optimizer, scheduler, loss, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m trainloader:\n\u001b[0;32m---> 29\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m     yout \u001b[39m=\u001b[39m model(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "epochs = 40\n",
    "\n",
    "optimizer = torch.optim.SGD(convnext_st_classic_atto.parameters(), \n",
    "                lr=1.0e-3,\n",
    "                momentum=0.9,\n",
    "                weight_decay=1e-4\n",
    "            )\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                max_lr=1.0e-3,\n",
    "                anneal_strategy='linear',\n",
    "                pct_start=0.4,\n",
    "                three_phase=True,\n",
    "                final_div_factor=500.0,\n",
    "                div_factor=25.0,\n",
    "                steps_per_epoch=len(trainloader), \n",
    "                epochs=epochs\n",
    "            )\n",
    "\n",
    "train(convnext_st_classic_atto, device, trainloader, testloader, optimizer, scheduler, loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: stem.0.weight\n",
      "1: stem.0.bias\n",
      "2: stem.1.weight\n",
      "3: stem.1.bias\n",
      "4: stages.0.downsample.0.weight\n",
      "5: stages.0.downsample.0.bias\n",
      "6: stages.0.downsample.1.weight\n",
      "7: stages.0.downsample.1.bias\n",
      "8: stages.0.blocks.0.conv_dw.weight\n",
      "9: stages.0.blocks.0.conv_dw.bias\n",
      "10: stages.0.blocks.0.norm.weight\n",
      "11: stages.0.blocks.0.norm.bias\n",
      "12: stages.0.blocks.0.mlp.0.weight\n",
      "13: stages.0.blocks.0.mlp.0.bias\n",
      "14: stages.0.blocks.0.mlp.2.weight\n",
      "15: stages.0.blocks.0.mlp.2.bias\n",
      "16: stages.0.blocks.0.scale.gamma\n",
      "17: stages.0.blocks.1.conv_dw.weight\n",
      "18: stages.0.blocks.1.conv_dw.bias\n",
      "19: stages.0.blocks.1.norm.weight\n",
      "20: stages.0.blocks.1.norm.bias\n",
      "21: stages.0.blocks.1.mlp.0.weight\n",
      "22: stages.0.blocks.1.mlp.0.bias\n",
      "23: stages.0.blocks.1.mlp.2.weight\n",
      "24: stages.0.blocks.1.mlp.2.bias\n",
      "25: stages.0.blocks.1.scale.gamma\n",
      "26: stages.1.downsample.0.weight\n",
      "27: stages.1.downsample.0.bias\n",
      "28: stages.1.downsample.1.weight\n",
      "29: stages.1.downsample.1.bias\n",
      "30: stages.1.blocks.0.conv_dw.weight\n",
      "31: stages.1.blocks.0.conv_dw.bias\n",
      "32: stages.1.blocks.0.norm.weight\n",
      "33: stages.1.blocks.0.norm.bias\n",
      "34: stages.1.blocks.0.mlp.0.weight\n",
      "35: stages.1.blocks.0.mlp.0.bias\n",
      "36: stages.1.blocks.0.mlp.2.weight\n",
      "37: stages.1.blocks.0.mlp.2.bias\n",
      "38: stages.1.blocks.0.scale.gamma\n",
      "39: stages.1.blocks.1.conv_dw.weight\n",
      "40: stages.1.blocks.1.conv_dw.bias\n",
      "41: stages.1.blocks.1.norm.weight\n",
      "42: stages.1.blocks.1.norm.bias\n",
      "43: stages.1.blocks.1.mlp.0.weight\n",
      "44: stages.1.blocks.1.mlp.0.bias\n",
      "45: stages.1.blocks.1.mlp.2.weight\n",
      "46: stages.1.blocks.1.mlp.2.bias\n",
      "47: stages.1.blocks.1.scale.gamma\n",
      "48: stages.2.downsample.0.weight\n",
      "49: stages.2.downsample.0.bias\n",
      "50: stages.2.downsample.1.weight\n",
      "51: stages.2.downsample.1.bias\n",
      "52: stages.2.blocks.0.conv_dw.weight\n",
      "53: stages.2.blocks.0.conv_dw.bias\n",
      "54: stages.2.blocks.0.norm.weight\n",
      "55: stages.2.blocks.0.norm.bias\n",
      "56: stages.2.blocks.0.mlp.0.weight\n",
      "57: stages.2.blocks.0.mlp.0.bias\n",
      "58: stages.2.blocks.0.mlp.2.weight\n",
      "59: stages.2.blocks.0.mlp.2.bias\n",
      "60: stages.2.blocks.0.scale.gamma\n",
      "61: stages.2.blocks.1.conv_dw.weight\n",
      "62: stages.2.blocks.1.conv_dw.bias\n",
      "63: stages.2.blocks.1.norm.weight\n",
      "64: stages.2.blocks.1.norm.bias\n",
      "65: stages.2.blocks.1.mlp.0.weight\n",
      "66: stages.2.blocks.1.mlp.0.bias\n",
      "67: stages.2.blocks.1.mlp.2.weight\n",
      "68: stages.2.blocks.1.mlp.2.bias\n",
      "69: stages.2.blocks.1.scale.gamma\n",
      "70: stages.2.blocks.2.conv_dw.weight\n",
      "71: stages.2.blocks.2.conv_dw.bias\n",
      "72: stages.2.blocks.2.norm.weight\n",
      "73: stages.2.blocks.2.norm.bias\n",
      "74: stages.2.blocks.2.mlp.0.weight\n",
      "75: stages.2.blocks.2.mlp.0.bias\n",
      "76: stages.2.blocks.2.mlp.2.weight\n",
      "77: stages.2.blocks.2.mlp.2.bias\n",
      "78: stages.2.blocks.2.scale.gamma\n",
      "79: stages.2.blocks.3.conv_dw.weight\n",
      "80: stages.2.blocks.3.conv_dw.bias\n",
      "81: stages.2.blocks.3.norm.weight\n",
      "82: stages.2.blocks.3.norm.bias\n",
      "83: stages.2.blocks.3.mlp.0.weight\n",
      "84: stages.2.blocks.3.mlp.0.bias\n",
      "85: stages.2.blocks.3.mlp.2.weight\n",
      "86: stages.2.blocks.3.mlp.2.bias\n",
      "87: stages.2.blocks.3.scale.gamma\n",
      "88: stages.2.blocks.4.conv_dw.weight\n",
      "89: stages.2.blocks.4.conv_dw.bias\n",
      "90: stages.2.blocks.4.norm.weight\n",
      "91: stages.2.blocks.4.norm.bias\n",
      "92: stages.2.blocks.4.mlp.0.weight\n",
      "93: stages.2.blocks.4.mlp.0.bias\n",
      "94: stages.2.blocks.4.mlp.2.weight\n",
      "95: stages.2.blocks.4.mlp.2.bias\n",
      "96: stages.2.blocks.4.scale.gamma\n",
      "97: stages.2.blocks.5.conv_dw.weight\n",
      "98: stages.2.blocks.5.conv_dw.bias\n",
      "99: stages.2.blocks.5.norm.weight\n",
      "100: stages.2.blocks.5.norm.bias\n",
      "101: stages.2.blocks.5.mlp.0.weight\n",
      "102: stages.2.blocks.5.mlp.0.bias\n",
      "103: stages.2.blocks.5.mlp.2.weight\n",
      "104: stages.2.blocks.5.mlp.2.bias\n",
      "105: stages.2.blocks.5.scale.gamma\n",
      "106: stages.3.downsample.0.weight\n",
      "107: stages.3.downsample.0.bias\n",
      "108: stages.3.downsample.1.weight\n",
      "109: stages.3.downsample.1.bias\n",
      "110: stages.3.blocks.0.conv_dw.weight\n",
      "111: stages.3.blocks.0.conv_dw.bias\n",
      "112: stages.3.blocks.0.norm.weight\n",
      "113: stages.3.blocks.0.norm.bias\n",
      "114: stages.3.blocks.0.mlp.0.weight\n",
      "115: stages.3.blocks.0.mlp.0.bias\n",
      "116: stages.3.blocks.0.mlp.2.weight\n",
      "117: stages.3.blocks.0.mlp.2.bias\n",
      "118: stages.3.blocks.0.scale.gamma\n",
      "119: stages.3.blocks.1.conv_dw.weight\n",
      "120: stages.3.blocks.1.conv_dw.bias\n",
      "121: stages.3.blocks.1.norm.weight\n",
      "122: stages.3.blocks.1.norm.bias\n",
      "123: stages.3.blocks.1.mlp.0.weight\n",
      "124: stages.3.blocks.1.mlp.0.bias\n",
      "125: stages.3.blocks.1.mlp.2.weight\n",
      "126: stages.3.blocks.1.mlp.2.bias\n",
      "127: stages.3.blocks.1.scale.gamma\n",
      "128: head.0.weight\n",
      "129: head.0.bias\n",
      "130: head.4.weight\n",
      "131: head.4.bias\n"
     ]
    }
   ],
   "source": [
    "# save layer names\n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(convnext_st_classic_atto.named_parameters()):\n",
    "    layer_names.append(name)\n",
    "    print(f'{idx}: {name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch20cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
