{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNeXt on FashionMNIST\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.5.0, llvm 15.0.4, commit 7b885c28, linux, python 3.10.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 07/10/23 14:31:31.391 170696] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n",
      "device = cuda\n",
      "convnext_st_classic_atto has 3,700,160 parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import timm.data\n",
    "import semitorch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "convnext_st_classic_atto = timm.create_model(\n",
    "    \"convnext_st_classic_atto\", in_chans=1, stem_patch_size=1\n",
    ")\n",
    "convnext_st_classic_atto = convnext_st_classic_atto.to(device)\n",
    "\n",
    "num_params = sum(\n",
    "    p.numel() for p in convnext_st_classic_atto.parameters() if p.requires_grad\n",
    ")\n",
    "print(f\"convnext_st_classic_atto has {num_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FashionMNIST dataset\n",
    "batch_size = 256\n",
    "num_workers = 8\n",
    "\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.286,),(0.353,)),\n",
    "        #transforms.RandomCrop((24, 24)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.286,),(0.353,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = FashionMNIST(root=\".\", train=True, download=True, transform=transforms_train)\n",
    "testset = FashionMNIST(root=\".\", train=False, download=True, transform=transforms_test)\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train the network with SGD+momentum under the [one cycle policy](https://arxiv.org/abs/1708.07120).\n",
    "As usual we use cross entropy loss.\n",
    "To track the track the training process we plot the accuracy of each training batch as a blue dot, every epoch we also test the network and the training data and plot the network's accuracy as an orange line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y):\n",
    "    with torch.no_grad():\n",
    "        yout = model(x)\n",
    "        _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "        return (y==prediction).sum().item() / float(y.numel())\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, testloader):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x = x.to(device)\n",
    "            accs.append(accuracy(model, x, y))\n",
    "    return sum(accs)/len(accs)\n",
    "\n",
    "\n",
    "def train(model, device, trainloader, testloader, optimizer, scheduler, loss, epochs):\n",
    "    accs = [] # list of accuracy on the test dataset for every epoch\n",
    "    trainaccs = [] # a list of the accuracies of all the training batches\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[6,4])\n",
    "    hdisplay = display.display(\"\", display_id=True)\n",
    "\n",
    "    for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        for x, y in trainloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yout = model(x)\n",
    "            _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "            trainaccs.append((y.cpu()==prediction).sum().item() / float(y.numel()))\n",
    "            l = loss(yout, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        accs.append(test(model, device, testloader))\n",
    "\n",
    "        ax.clear()\n",
    "        ax.set_xlim(0, epochs)\n",
    "        ax.set_ylim(0.75, 1.0)\n",
    "        ax.plot(np.linspace(0,len(accs),len(trainaccs)), \n",
    "            trainaccs, \".\", markersize=1.5, markerfacecolor=(0, 0, 1, 0.3))\n",
    "        ax.plot(np.linspace(1,len(accs),len(accs)), accs)\n",
    "        ax.text(0.6*epochs, 0.80, f\"max test acc = {max(accs):.2%}\", ha=\"center\", fontsize=10)\n",
    "        hdisplay.update(fig)\n",
    "        \n",
    "        # prevents OOM when GPU memory is tight\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    plt.close(fig)\n",
    "        \n",
    "\n",
    "def resetmodel(model: nn.Module) -> None:\n",
    "    @torch.no_grad()\n",
    "    def weight_reset(m: nn.Module):\n",
    "        reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "        if callable(reset_parameters):\n",
    "            m.reset_parameters()\n",
    "    model.apply(fn=weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convnext_st_classic_atto.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2bdc5e0a464a8c962e4a60b7d08316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "epochs = 40\n",
    "\n",
    "optimizer = torch.optim.AdamW(convnext_st_classic_atto.parameters(), \n",
    "                lr=2e-2,\n",
    "                weight_decay=0.01\n",
    "            )\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                max_lr=2e-2,\n",
    "                anneal_strategy='linear',\n",
    "                pct_start=0.4,\n",
    "                three_phase=True,\n",
    "                final_div_factor=500.0,\n",
    "                div_factor=25.0,\n",
    "                steps_per_epoch=len(trainloader), \n",
    "                epochs=epochs\n",
    "            )\n",
    "\n",
    "train(convnext_st_classic_atto, device, trainloader, testloader, optimizer, scheduler, loss, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: stem.0.weight\n",
      "1: stem.0.bias\n",
      "2: stem.1.weight\n",
      "3: stem.1.bias\n",
      "4: stages.0.downsample.0.weight\n",
      "5: stages.0.downsample.0.bias\n",
      "6: stages.0.downsample.1.weight\n",
      "7: stages.0.downsample.1.bias\n",
      "8: stages.0.blocks.0.conv_dw.weight\n",
      "9: stages.0.blocks.0.conv_dw.bias\n",
      "10: stages.0.blocks.0.norm.weight\n",
      "11: stages.0.blocks.0.norm.bias\n",
      "12: stages.0.blocks.0.mlp.0.weight\n",
      "13: stages.0.blocks.0.mlp.0.bias\n",
      "14: stages.0.blocks.0.mlp.2.weight\n",
      "15: stages.0.blocks.0.mlp.2.bias\n",
      "16: stages.0.blocks.0.scale.gamma\n",
      "17: stages.0.blocks.1.conv_dw.weight\n",
      "18: stages.0.blocks.1.conv_dw.bias\n",
      "19: stages.0.blocks.1.norm.weight\n",
      "20: stages.0.blocks.1.norm.bias\n",
      "21: stages.0.blocks.1.mlp.0.weight\n",
      "22: stages.0.blocks.1.mlp.0.bias\n",
      "23: stages.0.blocks.1.mlp.2.weight\n",
      "24: stages.0.blocks.1.mlp.2.bias\n",
      "25: stages.0.blocks.1.scale.gamma\n",
      "26: stages.1.downsample.0.weight\n",
      "27: stages.1.downsample.0.bias\n",
      "28: stages.1.downsample.1.weight\n",
      "29: stages.1.downsample.1.bias\n",
      "30: stages.1.blocks.0.conv_dw.weight\n",
      "31: stages.1.blocks.0.conv_dw.bias\n",
      "32: stages.1.blocks.0.norm.weight\n",
      "33: stages.1.blocks.0.norm.bias\n",
      "34: stages.1.blocks.0.mlp.0.weight\n",
      "35: stages.1.blocks.0.mlp.0.bias\n",
      "36: stages.1.blocks.0.mlp.2.weight\n",
      "37: stages.1.blocks.0.mlp.2.bias\n",
      "38: stages.1.blocks.0.scale.gamma\n",
      "39: stages.1.blocks.1.conv_dw.weight\n",
      "40: stages.1.blocks.1.conv_dw.bias\n",
      "41: stages.1.blocks.1.norm.weight\n",
      "42: stages.1.blocks.1.norm.bias\n",
      "43: stages.1.blocks.1.mlp.0.weight\n",
      "44: stages.1.blocks.1.mlp.0.bias\n",
      "45: stages.1.blocks.1.mlp.2.weight\n",
      "46: stages.1.blocks.1.mlp.2.bias\n",
      "47: stages.1.blocks.1.scale.gamma\n",
      "48: stages.2.downsample.0.weight\n",
      "49: stages.2.downsample.0.bias\n",
      "50: stages.2.downsample.1.weight\n",
      "51: stages.2.downsample.1.bias\n",
      "52: stages.2.blocks.0.conv_dw.weight\n",
      "53: stages.2.blocks.0.conv_dw.bias\n",
      "54: stages.2.blocks.0.norm.weight\n",
      "55: stages.2.blocks.0.norm.bias\n",
      "56: stages.2.blocks.0.mlp.0.weight\n",
      "57: stages.2.blocks.0.mlp.0.bias\n",
      "58: stages.2.blocks.0.mlp.2.weight\n",
      "59: stages.2.blocks.0.mlp.2.bias\n",
      "60: stages.2.blocks.0.scale.gamma\n",
      "61: stages.2.blocks.1.conv_dw.weight\n",
      "62: stages.2.blocks.1.conv_dw.bias\n",
      "63: stages.2.blocks.1.norm.weight\n",
      "64: stages.2.blocks.1.norm.bias\n",
      "65: stages.2.blocks.1.mlp.0.weight\n",
      "66: stages.2.blocks.1.mlp.0.bias\n",
      "67: stages.2.blocks.1.mlp.2.weight\n",
      "68: stages.2.blocks.1.mlp.2.bias\n",
      "69: stages.2.blocks.1.scale.gamma\n",
      "70: stages.2.blocks.2.conv_dw.weight\n",
      "71: stages.2.blocks.2.conv_dw.bias\n",
      "72: stages.2.blocks.2.norm.weight\n",
      "73: stages.2.blocks.2.norm.bias\n",
      "74: stages.2.blocks.2.mlp.0.weight\n",
      "75: stages.2.blocks.2.mlp.0.bias\n",
      "76: stages.2.blocks.2.mlp.2.weight\n",
      "77: stages.2.blocks.2.mlp.2.bias\n",
      "78: stages.2.blocks.2.scale.gamma\n",
      "79: stages.2.blocks.3.conv_dw.weight\n",
      "80: stages.2.blocks.3.conv_dw.bias\n",
      "81: stages.2.blocks.3.norm.weight\n",
      "82: stages.2.blocks.3.norm.bias\n",
      "83: stages.2.blocks.3.mlp.0.weight\n",
      "84: stages.2.blocks.3.mlp.0.bias\n",
      "85: stages.2.blocks.3.mlp.2.weight\n",
      "86: stages.2.blocks.3.mlp.2.bias\n",
      "87: stages.2.blocks.3.scale.gamma\n",
      "88: stages.2.blocks.4.conv_dw.weight\n",
      "89: stages.2.blocks.4.conv_dw.bias\n",
      "90: stages.2.blocks.4.norm.weight\n",
      "91: stages.2.blocks.4.norm.bias\n",
      "92: stages.2.blocks.4.mlp.0.weight\n",
      "93: stages.2.blocks.4.mlp.0.bias\n",
      "94: stages.2.blocks.4.mlp.2.weight\n",
      "95: stages.2.blocks.4.mlp.2.bias\n",
      "96: stages.2.blocks.4.scale.gamma\n",
      "97: stages.2.blocks.5.conv_dw.weight\n",
      "98: stages.2.blocks.5.conv_dw.bias\n",
      "99: stages.2.blocks.5.norm.weight\n",
      "100: stages.2.blocks.5.norm.bias\n",
      "101: stages.2.blocks.5.mlp.0.weight\n",
      "102: stages.2.blocks.5.mlp.0.bias\n",
      "103: stages.2.blocks.5.mlp.2.weight\n",
      "104: stages.2.blocks.5.mlp.2.bias\n",
      "105: stages.2.blocks.5.scale.gamma\n",
      "106: stages.3.downsample.0.weight\n",
      "107: stages.3.downsample.0.bias\n",
      "108: stages.3.downsample.1.weight\n",
      "109: stages.3.downsample.1.bias\n",
      "110: stages.3.blocks.0.conv_dw.weight\n",
      "111: stages.3.blocks.0.conv_dw.bias\n",
      "112: stages.3.blocks.0.norm.weight\n",
      "113: stages.3.blocks.0.norm.bias\n",
      "114: stages.3.blocks.0.mlp.0.weight\n",
      "115: stages.3.blocks.0.mlp.0.bias\n",
      "116: stages.3.blocks.0.mlp.2.weight\n",
      "117: stages.3.blocks.0.mlp.2.bias\n",
      "118: stages.3.blocks.0.scale.gamma\n",
      "119: stages.3.blocks.1.conv_dw.weight\n",
      "120: stages.3.blocks.1.conv_dw.bias\n",
      "121: stages.3.blocks.1.norm.weight\n",
      "122: stages.3.blocks.1.norm.bias\n",
      "123: stages.3.blocks.1.mlp.0.weight\n",
      "124: stages.3.blocks.1.mlp.0.bias\n",
      "125: stages.3.blocks.1.mlp.2.weight\n",
      "126: stages.3.blocks.1.mlp.2.bias\n",
      "127: stages.3.blocks.1.scale.gamma\n",
      "128: head.0.weight\n",
      "129: head.0.bias\n",
      "130: head.4.weight\n",
      "131: head.4.bias\n"
     ]
    }
   ],
   "source": [
    "# save layer names\n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(convnext_st_classic_atto.named_parameters()):\n",
    "    layer_names.append(name)\n",
    "    print(f'{idx}: {name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch20cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
