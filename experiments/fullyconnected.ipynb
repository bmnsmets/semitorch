{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Experiments\n",
    "\n",
    "\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import semitorch\n",
    "\n",
    "from collections import OrderedDict\n",
    "from IPython import display\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "data_path = os.path.abspath(\"./data\" if os.path.isdir(\"./data\") else \"../data\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"data path = {data_path}\\ndevice = {device}\")\n",
    "\n",
    "\n",
    "def split_dataset(dataset: torch.utils.data.Dataset):\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    return torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load iris data\n",
    "[https://www.kaggle.com/datasets/uciml/iris](https://www.kaggle.com/datasets/uciml/iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv(\n",
    "    os.path.join(data_path, \"iris.csv\"), index_col=0, dtype={\"Species\": \"string\"}\n",
    ")\n",
    "iris_x = torch.Tensor(iris_df.iloc[:, [0, 1, 2, 3]].to_numpy()).to(device)\n",
    "\n",
    "iris_y = (\n",
    "    iris_df[\"Species\"]\n",
    "    .map({\n",
    "        \"Iris-setosa\": 0,\n",
    "        \"Iris-versicolor\": 1,\n",
    "        \"Iris-virginica\": 2,\n",
    "    }).to_numpy()\n",
    ")\n",
    "iris_y = torch.Tensor(iris_y).to(torch.int64).to(device)\n",
    "print(f\"Iris dataset: input features = {iris_x.shape[1]}, classes = {torch.unique(iris_y).shape[0]}, samples = {len(iris_y)}\")\n",
    "\n",
    "# normalize\n",
    "torch.nn.functional.normalize(iris_x, dim=0, out=iris_x)\n",
    "\n",
    "iris_train, iris_test = split_dataset(torch.utils.data.TensorDataset(iris_x, iris_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load heart disease data\n",
    "\n",
    "[https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv(os.path.join(data_path, \"heart.csv\"))\n",
    "heart_x = (torch.Tensor(heart_df.iloc[:, range(13)].to_numpy()).to(torch.float32).to(device))\n",
    "heart_y = torch.Tensor(heart_df.iloc[:, -1].to_numpy()).to(torch.int64).to(device)\n",
    "\n",
    "print(f\"Heart disease dataset: input features = {heart_x.shape[1]}, classes = {torch.unique(heart_y).shape[0]}, samples = {len(heart_y)}\")\n",
    "\n",
    "# normalize\n",
    "torch.nn.functional.normalize(heart_x, dim=0, out=heart_x)\n",
    "\n",
    "heart_train, heart_test = split_dataset(torch.utils.data.TensorDataset(heart_x, heart_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Naitzat data\n",
    "\n",
    "[https://github.com/topnn/topnn_framework](https://github.com/topnn/topnn_framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles_x, circles_y = torch.load(os.path.join(data_path, \"naitzat\", \"circles_type_8.pt\"))\n",
    "circles_train, circles_test = split_dataset(torch.utils.data.TensorDataset(circles_x, circles_y))\n",
    "print(f\"Circles dataset: input features = {circles_x.shape[1]}, classes = {torch.unique(circles_y).shape[0]}, samples = {len(circles_y)}\")\n",
    "\n",
    "rings_x, rings_y = torch.load(os.path.join(data_path, \"naitzat\", \"rings_9.pt\"))\n",
    "rings_train, rings_test = split_dataset(torch.utils.data.TensorDataset(rings_x, rings_y))\n",
    "print(f\"Rings dataset: input features = {rings_x.shape[1]}, classes = {torch.unique(rings_y).shape[0]}, samples = {len(rings_y)}\")\n",
    "\n",
    "spheres_x, spheres_y = torch.load(os.path.join(data_path, \"naitzat\", \"spheres_9.pt\"))\n",
    "spheres_train, spheres_test = split_dataset(torch.utils.data.TensorDataset(spheres_x, spheres_y))\n",
    "print(f\"Spheres dataset: input features = {spheres_x.shape[1]}, classes = {torch.unique(spheres_y).shape[0]}, samples = {len(spheres_y)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name: str, num_features: int, num_classes: int):\n",
    "    BACKBONE_FEATURES = 16\n",
    "\n",
    "    if model_name == \"linear_relu\":\n",
    "        backbone = nn.Sequential(\n",
    "            nn.Linear(num_features, BACKBONE_FEATURES),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(BACKBONE_FEATURES, BACKBONE_FEATURES),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    elif model_name == \"linear_maxplus\":\n",
    "        backbone = nn.Sequential(\n",
    "            nn.Linear(num_features, BACKBONE_FEATURES),\n",
    "            semitorch.MaxPlus(BACKBONE_FEATURES, BACKBONE_FEATURES),\n",
    "        )\n",
    "    elif model_name == \"linear_minplus\":\n",
    "        backbone = nn.Sequential(\n",
    "            nn.Linear(num_features, BACKBONE_FEATURES),\n",
    "            semitorch.MinPlus(BACKBONE_FEATURES, BACKBONE_FEATURES),\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown model ({model_name})\")\n",
    "\n",
    "    head = nn.Linear(BACKBONE_FEATURES, num_classes, bias=False)\n",
    "    model = nn.Sequential(OrderedDict([(\"backbone\", backbone), (\"head\", head)]))\n",
    "\n",
    "    model.name = model_name\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y):\n",
    "    with torch.no_grad():\n",
    "        yout = model(x)\n",
    "        _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "\n",
    "        return (y.cpu() == prediction).sum().item() / float(y.numel())\n",
    "\n",
    "\n",
    "def test(model, device, testloader):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x = x.to(device)\n",
    "            accs.append(accuracy(model, x, y))\n",
    "\n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "\n",
    "def train(model, device, trainloader, testloader, optimizer, scheduler, loss, epochs):\n",
    "    accs = []  # list of accuracy on the test dataset for every epoch\n",
    "    trainaccs = []  # a list of the accuracies of all the training batches\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[6, 4])\n",
    "    hdisplay = display.display(\"\", display_id=True)\n",
    "\n",
    "    for _ in trange(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for x, y in trainloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yout = model(x)\n",
    "            _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "            trainaccs.append((y.cpu() == prediction).sum().item() / float(y.numel()))\n",
    "\n",
    "            l = loss(yout, y.squeeze())\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        accs.append(test(model, device, testloader))\n",
    "\n",
    "        ax.clear()\n",
    "        ax.set_xlim(0, epochs)\n",
    "        ax.set_ylim(-0.02, 1.02)\n",
    "        ax.plot(\n",
    "            np.linspace(0, len(accs), len(trainaccs)),\n",
    "            trainaccs,\n",
    "            \".\",\n",
    "            markersize=1.5,\n",
    "            markerfacecolor=(0, 0, 1, 0.3),\n",
    "        )\n",
    "        ax.plot(np.linspace(1, len(accs), len(accs)), accs)\n",
    "        ax.text(\n",
    "            0.6 * epochs,\n",
    "            0.80,\n",
    "            f\"max test acc = {max(accs):.2%}\",\n",
    "            ha=\"center\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "        hdisplay.update(fig)\n",
    "\n",
    "        # prevents OOM when GPU memory is tight\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def resetmodel(model: nn.Module) -> None:\n",
    "    @torch.no_grad()\n",
    "    def weight_reset(m: nn.Module):\n",
    "        reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "        if callable(reset_parameters):\n",
    "            m.reset_parameters()\n",
    "\n",
    "    model.apply(fn=weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train_loader = DataLoader(iris_train, batch_size=8, shuffle=True)\n",
    "iris_test_loader = DataLoader(iris_test, batch_size=len(iris_test), shuffle=False)\n",
    "\n",
    "heart_train_loader = DataLoader(heart_train, batch_size=32, shuffle=True)\n",
    "heart_test_loader = DataLoader(heart_test, batch_size=len(heart_test))\n",
    "\n",
    "circles_train_loader = DataLoader(circles_train, batch_size=16, shuffle=True)\n",
    "circles_test_loader = DataLoader(circles_test, batch_size=len(circles_test), shuffle=True)\n",
    "\n",
    "rings_train_loader = DataLoader(rings_train, batch_size=16, shuffle=True)\n",
    "rings_test_loader = DataLoader(rings_test, batch_size=len(rings_test), shuffle=True)\n",
    "\n",
    "spheres_train_loader = DataLoader(spheres_train, batch_size=16, shuffle=True)\n",
    "spheres_test_loader = DataLoader(spheres_test, batch_size=len(spheres_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_models = [\n",
    "    create_model(\"linear_relu\", 4, 3).to(device),\n",
    "    create_model(\"linear_maxplus\", 4, 3).to(device),\n",
    "    create_model(\"linear_minplus\", 4, 3).to(device),\n",
    "]\n",
    "\n",
    "for model in iris_models:\n",
    "    model_ref = model.name.replace(\"_\", \"/\")\n",
    "    print(f\"{model_ref} model has {count_parameters(model)} parameters\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "\n",
    "for model in iris_models:\n",
    "    if model.name == \"linear_relu\":\n",
    "        lr: float = 2e-2\n",
    "    elif model.name == \"linear_maxplus\":\n",
    "        lr: float = 2e-4\n",
    "    elif model.name == \"linear_minplus\":\n",
    "        lr: float = 2e-4\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown model ({model.name})\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        anneal_strategy=\"linear\",\n",
    "        pct_start=0.3,\n",
    "        three_phase=True,\n",
    "        final_div_factor=1000.0,\n",
    "        div_factor=10.0,\n",
    "        steps_per_epoch=len(iris_train),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        device,\n",
    "        iris_train_loader,\n",
    "        iris_test_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_models = [\n",
    "    create_model(\"linear_relu\", 13, 12).to(device),\n",
    "    create_model(\"linear_maxplus\", 13, 12).to(device),\n",
    "    create_model(\"linear_minplus\", 13, 12).to(device),\n",
    "]\n",
    "\n",
    "for model in heart_models:\n",
    "    model_ref = model.name.replace(\"_\", \"/\")\n",
    "    print(f\"{model_ref} model has {count_parameters(model)} parameters\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "\n",
    "for model in heart_models:\n",
    "    if model.name == \"linear_relu\":\n",
    "        lr: float = 2e-2\n",
    "    elif model.name == \"linear_maxplus\":\n",
    "        lr: float = 2e-4\n",
    "    elif model.name == \"linear_minplus\":\n",
    "        lr: float = 2e-4\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown model ({model.name})\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        anneal_strategy=\"linear\",\n",
    "        pct_start=0.3,\n",
    "        three_phase=True,\n",
    "        final_div_factor=1000.0,\n",
    "        div_factor=10.0,\n",
    "        steps_per_epoch=len(heart_train),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        device,\n",
    "        heart_train_loader,\n",
    "        heart_test_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "circles_models = [\n",
    "    create_model(\"linear_relu\", 2, 2).to(device),\n",
    "    create_model(\"linear_maxplus\", 2, 2).to(device),\n",
    "    create_model(\"linear_minplus\", 2, 2).to(device),\n",
    "]\n",
    "\n",
    "for model in circles_models:\n",
    "    model_ref = model.name.replace(\"_\", \"/\")\n",
    "    print(f\"{model_ref} model has {count_parameters(model)} parameters\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "\n",
    "for model in circles_models:\n",
    "    if model.name == \"linear_relu\":\n",
    "        lr: float = 2e-2\n",
    "    elif model.name == \"linear_maxplus\":\n",
    "        lr: float = 2e-4\n",
    "    elif model.name == \"linear_minplus\":\n",
    "        lr: float = 2e-4\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown model ({model.name})\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        anneal_strategy=\"linear\",\n",
    "        pct_start=0.3,\n",
    "        three_phase=True,\n",
    "        final_div_factor=1000.0,\n",
    "        div_factor=10.0,\n",
    "        steps_per_epoch=len(circles_train),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        device,\n",
    "        circles_train_loader,\n",
    "        circles_test_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        epochs,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rings_models = [\n",
    "    create_model(\"linear_relu\", 3, 2).to(device),\n",
    "    create_model(\"linear_maxplus\", 3, 2).to(device),\n",
    "    create_model(\"linear_minplus\", 3, 2).to(device),\n",
    "]\n",
    "\n",
    "for model in rings_models:\n",
    "    model_ref = model.name.replace(\"_\", \"/\")\n",
    "    print(f\"{model_ref} model has {count_parameters(model)} parameters\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "\n",
    "for model in rings_models:\n",
    "    if model.name == \"linear_relu\":\n",
    "        lr: float = 2e-2\n",
    "    elif model.name == \"linear_maxplus\":\n",
    "        lr: float = 2e-4\n",
    "    elif model.name == \"linear_minplus\":\n",
    "        lr: float = 2e-4\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown model ({model.name})\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        anneal_strategy=\"linear\",\n",
    "        pct_start=0.3,\n",
    "        three_phase=True,\n",
    "        final_div_factor=1000.0,\n",
    "        div_factor=10.0,\n",
    "        steps_per_epoch=len(rings_train),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        device,\n",
    "        rings_train_loader,\n",
    "        rings_test_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        epochs,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spheres_models = [\n",
    "    create_model(\"linear_relu\", 3, 2).to(device),\n",
    "    create_model(\"linear_maxplus\", 3, 2).to(device),\n",
    "    create_model(\"linear_minplus\", 3, 2).to(device),\n",
    "]\n",
    "\n",
    "for model in rings_models:\n",
    "    model_ref = model.name.replace(\"_\", \"/\")\n",
    "    print(f\"{model_ref} model has {count_parameters(model)} parameters\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "\n",
    "for model in spheres_models:\n",
    "    if model.name == \"linear_relu\":\n",
    "        lr: float = 2e-2\n",
    "    elif model.name == \"linear_maxplus\":\n",
    "        lr: float = 2e-4\n",
    "    elif model.name == \"linear_minplus\":\n",
    "        lr: float = 2e-4\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown model ({model.name})\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        anneal_strategy=\"linear\",\n",
    "        pct_start=0.3,\n",
    "        three_phase=True,\n",
    "        final_div_factor=1000.0,\n",
    "        div_factor=10.0,\n",
    "        steps_per_epoch=len(spheres_train),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        device,\n",
    "        spheres_train_loader,\n",
    "        spheres_test_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        epochs,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
