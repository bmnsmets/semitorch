{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Experiments: Fashion MNIST\n",
    "\n",
    "This notebook contains the experiments with fully connected neural networks on [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.5.0, llvm 15.0.4, commit 7b885c28, linux, python 3.10.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 09/25/23 17:12:27.951 334235] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=cuda\n",
      "data path = /home/bmnsmets/Documents/semitorch/data\n",
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import semitorch\n",
    "from semitorch import MultiLRScheduler, MultiOptimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from typing import Union, Tuple\n",
    "\n",
    "# Reproducibility\n",
    "RNG_SEED = 43\n",
    "torch.manual_seed(RNG_SEED)\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed()\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "data_path = os.path.abspath(\"./data\" if os.path.isdir(\"./data\") else \"../data\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"data path = {data_path}\\ndevice = {device}\")\n",
    "\n",
    "\n",
    "def split_dataset(dataset: torch.utils.data.Dataset, train_ratio: float = 0.5):\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    return torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def resetmodel(model: nn.Module) -> None:\n",
    "    @torch.no_grad()\n",
    "    def weight_reset(m: nn.Module):\n",
    "        reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "        if callable(reset_parameters):\n",
    "            m.reset_parameters()\n",
    "\n",
    "    model.apply(fn=weight_reset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load FashionMNIST data\n",
    "\n",
    "https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNIST dataset: input features = 16x16, classes = 10, samples = 60000\n"
     ]
    }
   ],
   "source": [
    "# Load FashionMNIST dataset\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.286,), (0.353,)),\n",
    "        transforms.Resize((16, 16), antialias=True),\n",
    "        # transforms.RandomResizedCrop(\n",
    "        #     (16, 16), scale=(0.9, 1.0), ratio=(0.9, 1.1), antialias=True\n",
    "        # ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.286,), (0.353,)),\n",
    "        transforms.Resize((16, 16), antialias=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fashion_train = FashionMNIST(\n",
    "    root=data_path, train=True, download=True, transform=transforms_train\n",
    ")\n",
    "fashion_test = FashionMNIST(\n",
    "    root=data_path, train=False, download=True, transform=transforms_test\n",
    ")\n",
    "\n",
    "fashion_num_features = fashion_test[0][0].shape[1] * fashion_test[0][0].shape[1]\n",
    "fashion_num_classes = torch.unique(fashion_test.targets).shape[0]\n",
    "\n",
    "print(\n",
    "    f\"FashionMNIST dataset: input features = {fashion_test[0][0].shape[1]}x{fashion_test[0][0].shape[1]}, classes = {torch.unique(fashion_test.targets).shape[0]}, samples = {len(fashion_train)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(RNG_SEED)\n",
    "\n",
    "fashion_train_loader = DataLoader(\n",
    "    fashion_train,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    # num_workers=4,\n",
    ")\n",
    "fashion_test_loader = DataLoader(fashion_test, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Baseline linear-ReLU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearReLU(nn.Module):\n",
    "    def __init__(self, num_features: int, num_classes: int, width: int = 16):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Linear(num_features, width, bias=False)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(width, width, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(width, width, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(width, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.stem(x)\n",
    "        y = y + self.layer1(y)\n",
    "        y = y + self.layer2(y)\n",
    "        return self.head(y)\n",
    "\n",
    "    def get_optimizer_and_scheduler(self, epochs: int, steps_per_epoch: int):\n",
    "        lr = 8e-3\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=0.01)\n",
    "        #optimizer = torch.optim.SGD(self.parameters(), lr=lr, weight_decay=0.01, momentum=0.95)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=lr,\n",
    "            anneal_strategy=\"linear\",\n",
    "            pct_start=0.4,\n",
    "            three_phase=True,\n",
    "            final_div_factor=1000.0,\n",
    "            div_factor=10.0,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tropical networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMaxPlus(nn.Module):\n",
    "    def __init__(self, num_features: int, num_classes: int, width: int = 16):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Linear(num_features, width, bias=False)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(width, width // 2, bias=False),\n",
    "            semitorch.MaxPlus(width // 2, width, bias=False),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(width, width // 2, bias=False),\n",
    "            semitorch.MaxPlus(width // 2, width, bias=False),\n",
    "        )\n",
    "        self.head = nn.Linear(width, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.stem(x)\n",
    "        y = y + self.layer1(y)\n",
    "        y = y + self.layer2(y)\n",
    "        return self.head(y)\n",
    "\n",
    "    def get_optimizer_and_scheduler(self, epochs: int, steps_per_epoch: int):\n",
    "        return tropcial_optimizer_and_scheduler(self, epochs, steps_per_epoch)\n",
    "\n",
    "\n",
    "class LinearMinPlus(nn.Module):\n",
    "    def __init__(self, num_features: int, num_classes: int, width: int = 16):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Linear(num_features, width, bias=False)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(width, width // 2, bias=False),\n",
    "            semitorch.MinPlus(width // 2, width, bias=False),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(width, width // 2, bias=False),\n",
    "            semitorch.MinPlus(width // 2, width, bias=False),\n",
    "        )\n",
    "        self.head = nn.Linear(width, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.stem(x)\n",
    "        y = y + self.layer1(y)\n",
    "        y = y + self.layer2(y)\n",
    "        return self.head(y)\n",
    "\n",
    "    def get_optimizer_and_scheduler(self, epochs: int, steps_per_epoch: int):\n",
    "        return tropcial_optimizer_and_scheduler(self, epochs, steps_per_epoch)\n",
    "\n",
    "\n",
    "def tropcial_optimizer_and_scheduler(\n",
    "    model: Union[LinearMaxPlus, LinearMinPlus], epochs: int, steps_per_epoch: int\n",
    "):\n",
    "    lin_lr = 8e-3\n",
    "    trop_lr = 1e-2\n",
    "    linear_params = chain(\n",
    "        model.stem.parameters(),\n",
    "        model.layer1[0].parameters(),\n",
    "        model.layer2[0].parameters(),\n",
    "        model.head.parameters(),\n",
    "    )\n",
    "    tropical_params = chain(model.layer1[1].parameters(), model.layer2[1].parameters())\n",
    "    opt1 = torch.optim.AdamW(linear_params, lr=lin_lr, weight_decay=0.01)\n",
    "    #opt1 = torch.optim.SGD(linear_params, lr=lin_lr, weight_decay=0.01, momentum=0.95)\n",
    "    sch1 = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        opt1,\n",
    "        max_lr=lin_lr,\n",
    "        anneal_strategy=\"linear\",\n",
    "        pct_start=0.45,\n",
    "        three_phase=True,\n",
    "        final_div_factor=1000.0,\n",
    "        div_factor=10.0,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    opt2 = torch.optim.AdamW(tropical_params, lr=trop_lr, weight_decay=0.01)\n",
    "    #opt2 = torch.optim.SGD(tropical_params, lr=trop_lr, weight_decay=0.01, momentum=0.95)\n",
    "    sch2 = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        opt2,\n",
    "        max_lr=trop_lr,\n",
    "        anneal_strategy=\"linear\",\n",
    "        pct_start=0.4,\n",
    "        three_phase=True,\n",
    "        final_div_factor=1000.0,\n",
    "        div_factor=10.0,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    optimizer = MultiOptimizer(opt1, opt2)\n",
    "    scheduler = MultiLRScheduler(sch1, sch2)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logarithmic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLogPlus(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_features: int, num_classes: int, width: int = 16, mu: float = 1.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.stem = nn.Linear(num_features, width, bias=False)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(width, width // 2, bias=False),\n",
    "            semitorch.LogPlus(width // 2, width, mu=mu, bias=False),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(width, width // 2, bias=False),\n",
    "            semitorch.LogPlus(width // 2, width, mu=mu, bias=False),\n",
    "        )\n",
    "        self.head = nn.Linear(width, num_classes, bias=False)\n",
    "        # self.grad_clamp = 3e-3\n",
    "        # linear_params = chain(\n",
    "        #     self.stem.parameters(),\n",
    "        #     self.layer1[0].parameters(),\n",
    "        #     self.layer2[0].parameters(),\n",
    "        #     self.head.parameters(),\n",
    "        # )\n",
    "        # for p in linear_params:\n",
    "        #     p.register_hook(lambda g: torch.clamp(g, -self.grad_clamp, self.grad_clamp))\n",
    "        # f = open(\"log.txt\", \"w\")\n",
    "        # def pr(x):\n",
    "        #     print(f\"{torch.linalg.vector_norm(x, ord=torch.inf)}\", file=f)\n",
    "        #     return x\n",
    "        # for p in self.parameters():\n",
    "        #     p.register_hook(pr)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.stem(x)\n",
    "        y = y + self.layer1(y)\n",
    "        y = y + self.layer2(y)\n",
    "        return self.head(y)\n",
    "\n",
    "    def get_optimizer_and_scheduler(self, epochs: int, steps_per_epoch: int):\n",
    "        lin_lr = 8e-3 #4e-4 #/ np.abs(self.mu)\n",
    "        log_lr = 1e-2 #4e-4 #/ np.abs(self.mu)\n",
    "        linear_params = chain(\n",
    "            self.stem.parameters(),\n",
    "            self.layer1[0].parameters(),\n",
    "            self.layer2[0].parameters(),\n",
    "            self.head.parameters(),\n",
    "        )\n",
    "        log_params = chain(self.layer1[1].parameters(), self.layer2[1].parameters())\n",
    "        opt1 = torch.optim.AdamW(linear_params, lr=lin_lr, weight_decay=0.01)\n",
    "        #opt1 = torch.optim.SGD(linear_params, lr=lin_lr, weight_decay=0.01, momentum=0.95)\n",
    "        sch1 = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt1,\n",
    "            max_lr=lin_lr,\n",
    "            anneal_strategy=\"linear\",\n",
    "            pct_start=0.45,\n",
    "            three_phase=True,\n",
    "            final_div_factor=10.0,\n",
    "            div_factor=1000.0,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            cycle_momentum=False,\n",
    "        )\n",
    "        opt2 = torch.optim.AdamW(log_params, lr=log_lr, weight_decay=0.01)\n",
    "        #opt2 = torch.optim.SGD(log_params, lr=log_lr, weight_decay=0.01, momentum=0.95)\n",
    "        sch2 = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            opt2,\n",
    "            max_lr=log_lr,\n",
    "            anneal_strategy=\"linear\",\n",
    "            pct_start=0.45,\n",
    "            three_phase=True,\n",
    "            final_div_factor=10.0,\n",
    "            div_factor=1000.0,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            cycle_momentum=False,\n",
    "        )\n",
    "        optimizer = MultiOptimizer(opt1, opt2)\n",
    "        scheduler = MultiLRScheduler(sch1, sch2)\n",
    "        return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y):\n",
    "    with torch.no_grad():\n",
    "        yout = model(x)\n",
    "        _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "        return (y.cpu() == prediction).sum().item() / float(y.numel())\n",
    "\n",
    "\n",
    "def test(model, device, testloader):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x = torch.flatten(x, -3, -1)\n",
    "            x = x.to(device)\n",
    "            accs.append(accuracy(model, x, y))\n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "\n",
    "def train(model, device, trainloader, testloader, optimizer, scheduler, loss, epochs):\n",
    "    accs = []  # list of accuracy on the test dataset for every epoch\n",
    "    trainaccs = []  # a list of the accuracies of all the training batches\n",
    "    epoch_len = len(trainloader)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[4, 3])\n",
    "    hdisplay = display.display(\"\", display_id=True)\n",
    "\n",
    "    for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        for x, y in trainloader:\n",
    "            x = torch.flatten(x, -3, -1)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yout = model(x)\n",
    "            _, prediction = torch.max(yout.cpu(), dim=1)\n",
    "            trainaccs.append((y.cpu() == prediction).sum().item() / float(y.numel()))\n",
    "            l = loss(yout, y)\n",
    "            if l.isnan().any().item():\n",
    "                print(f\"yout.isnan().any()={yout.cpu().isnan().any().item()}\")\n",
    "                print(f\"yout.isinf().any()={yout.cpu().isinf().any().item()}\")\n",
    "                print(f\"torch.max(yout)={torch.max(yout)}\")\n",
    "                print(f\"torch.min(yout)={torch.min(yout)}\")\n",
    "                print(f\"yout={yout.cpu()}\")\n",
    "                print(f\"loss={l.cpu()}\")\n",
    "                raise Exception(\"loss output has NaN\")\n",
    "            l.backward()\n",
    "            for p in model.parameters():\n",
    "                if p.grad.isnan().any().item():\n",
    "                    raise Exception(\"NaN found in grads\")\n",
    "            optimizer.step()\n",
    "            if scheduler != None:\n",
    "                scheduler.step()\n",
    "\n",
    "        accs.append(test(model, device, testloader))\n",
    "\n",
    "        ax.clear()\n",
    "        ax.set_xlim(0, epochs)\n",
    "        ax.set_ylim(0.58, 1.02)\n",
    "        ax.plot(\n",
    "            np.linspace(0, len(accs), len(trainaccs)),\n",
    "            trainaccs,\n",
    "            \".\",\n",
    "            markersize=1.5,\n",
    "            markerfacecolor=(0, 0, 1, 0.3),\n",
    "        )\n",
    "        ax.plot(np.linspace(1, len(accs), len(accs)), accs)\n",
    "        ax.text(\n",
    "            0.55 * epochs,\n",
    "            0.65,\n",
    "            f\"max test acc = {max(accs):.2%}\",\n",
    "            ha=\"center\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "        train100acc = sum(trainaccs[-epoch_len:]) / len(trainaccs[-epoch_len:])\n",
    "        ax.text(\n",
    "            0.55 * epochs,\n",
    "            0.70,\n",
    "            f\"train acc (last epoch) = {train100acc:.2%}\",\n",
    "            ha=\"center\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "        hdisplay.update(fig)\n",
    "\n",
    "        # prevents OOM when GPU memory is tight\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    plt.close(fig)\n",
    "    return sum(trainaccs[-epoch_len:]) / len(trainaccs[-epoch_len:]), max(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "model = fashion_linear_logplus_m10:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f228727f82a245da9de17e5f5841fe1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m optimizer, scheduler \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mget_optimizer_and_scheduler(epochs, \u001b[39mlen\u001b[39m(fashion_train_loader))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mmodel = \u001b[39m\u001b[39m{\u001b[39;00mm\u001b[39m.\u001b[39mtag\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m train100acc, testacc \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     device,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     fashion_train_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     fashion_test_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     scheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     loss,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     epochs\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m fashion_train100accs\u001b[39m.\u001b[39mappend(train100acc)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m fashion_testaccs\u001b[39m.\u001b[39mappend(testacc)\n",
      "\u001b[1;32m/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m yout \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m _, prediction \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(yout\u001b[39m.\u001b[39mcpu(), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m trainaccs\u001b[39m.\u001b[39mappend((y\u001b[39m.\u001b[39mcpu() \u001b[39m==\u001b[39m prediction)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(y\u001b[39m.\u001b[39mnumel()))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch200cu118/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstem(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     y \u001b[39m=\u001b[39m y \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     y \u001b[39m=\u001b[39m y \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bmnsmets/Documents/semitorch/experiments/article-fullyconnected-fashion.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(y)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch200cu118/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch200cu118/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch200cu118/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/semitorch/src/semitorch/logplus.py:78\u001b[0m, in \u001b[0;36mLogPlus.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m logplus(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmu, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Documents/semitorch/src/semitorch/logplus.py:22\u001b[0m, in \u001b[0;36m_logplus\u001b[0;34m(x, w, mu, bias)\u001b[0m\n\u001b[1;32m     20\u001b[0m     bias \u001b[39m=\u001b[39m bias\u001b[39m.\u001b[39mmul(mu)\u001b[39m.\u001b[39mexp()\n\u001b[1;32m     21\u001b[0m y \u001b[39m=\u001b[39m linear(x, w, bias)\n\u001b[0;32m---> 22\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misclose(y\u001b[39m.\u001b[39mcpu(), Tensor([\u001b[39m0.0\u001b[39m]), atol\u001b[39m=\u001b[39m\u001b[39m1e-45\u001b[39m, rtol\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInf in logplus output: max=\u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mmax(y\u001b[39m.\u001b[39mabs())\u001b[39m}\u001b[39;00m\u001b[39m min=\u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mmin(y\u001b[39m.\u001b[39mabs())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mlog()\u001b[39m.\u001b[39mdiv(mu)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fashion_model_width = 8 # must be even for the number of parameters to match\n",
    "\n",
    "fashion_linear_relu = LinearReLU(\n",
    "    fashion_num_features, fashion_num_classes, width=fashion_model_width\n",
    ")\n",
    "fashion_linear_relu.tag = \"fashion_linear_relu\"\n",
    "\n",
    "fashion_linear_maxplus = LinearMaxPlus(\n",
    "    fashion_num_features, fashion_num_classes, width=fashion_model_width\n",
    ")\n",
    "fashion_linear_maxplus.tag = \"fashion_linear_maxplus\"\n",
    "\n",
    "fashion_linear_minplus = LinearMinPlus(\n",
    "    fashion_num_features, fashion_num_classes, width=fashion_model_width\n",
    ")\n",
    "fashion_linear_minplus.tag = \"fashion_linear_minplus\"\n",
    "\n",
    "fashion_linear_logplus_m10 = LinearLogPlus(\n",
    "    fashion_num_features, fashion_num_classes, width=fashion_model_width, mu=-10\n",
    ")\n",
    "fashion_linear_logplus_m10.tag = \"fashion_linear_logplus_m10\"\n",
    "\n",
    "fashion_linear_logplus_m1 = LinearLogPlus(\n",
    "    fashion_num_features, fashion_num_classes, width=fashion_model_width, mu=-1\n",
    ")\n",
    "fashion_linear_logplus_m1.tag = \"fashion_linear_logplust_m1\"\n",
    "\n",
    "fashion_linear_logplus_p1 = LinearLogPlus(\n",
    "    fashion_num_features, fashion_num_classes, width=fashion_model_width, mu=1\n",
    ")\n",
    "fashion_linear_logplus_p1.tag = \"fashion_linear_logplust_p1\"\n",
    "\n",
    "fashion_linear_logplus_p10 = LinearLogPlus(\n",
    "    fashion_num_features, fashion_num_classes, width=fashion_model_width, mu=10\n",
    ")\n",
    "fashion_linear_logplus_p10.tag = \"fashion_linear_logplust_p10\"\n",
    "\n",
    "fashion_models = [\n",
    "    #fashion_linear_relu,\n",
    "    #fashion_linear_maxplus,\n",
    "    #fashion_linear_minplus,\n",
    "    fashion_linear_logplus_m10,\n",
    "    #fashion_linear_logplus_m1,\n",
    "    #fashion_linear_logplus_p1,\n",
    "    #fashion_linear_logplus_p10,\n",
    "]\n",
    "\n",
    "fashion_models = [m.to(device) for m in fashion_models]\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "epochs = 40\n",
    "\n",
    "fashion_train100accs = []\n",
    "fashion_testaccs = []\n",
    "for im, m in enumerate(fashion_models):\n",
    "    optimizer, scheduler = m.get_optimizer_and_scheduler(epochs, len(fashion_train_loader))\n",
    "    print(f\"\\n\\nmodel = {m.tag}:\")\n",
    "    train100acc, testacc = train(\n",
    "        m,\n",
    "        device,\n",
    "        fashion_train_loader,\n",
    "        fashion_test_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        epochs\n",
    "    )\n",
    "    fashion_train100accs.append(train100acc)\n",
    "    fashion_testaccs.append(testacc)\n",
    "\n",
    "print(f\"train accuracies (last epoch): {fashion_train100accs}\")\n",
    "print(f\"test accuracies: {fashion_testaccs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for p in fashion_linear_logplus_m10.parameters():\n",
    "    print(torch.isnan(p).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:0')\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for p in fashion_linear_logplus_m10.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch200cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
